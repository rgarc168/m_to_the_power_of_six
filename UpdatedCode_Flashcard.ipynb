{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a8a9f3-8df8-4262-aa8c-8d882230e1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poojapal/.local/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading NousResearch/Hermes-2-Pro-Mistral-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d527be03b3419db37531b8d592b522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM loaded and ready!\n",
      "üöÄ Initializing RAG system...\n",
      "üîÑ Initializing RAG system...\n",
      "‚úÖ Loaded cached RAG with 437 chunks\n",
      "\n",
      "üöÄ Creating AI Tutor interface...\n",
      "üéâ AI Tutor ready!\n",
      "============================================================\n",
      "‚úÖ Complete Learning System Available:\n",
      "  üìö Learning Mode - Structured topic-based learning paths\n",
      "  üéì Tutor Mode - Socratic Q&A with guided discovery\n",
      "  üéÆ Game Mode - XP-based practice with flashcards, quizzes & coding\n",
      "  üåü Dynamic content generation with contextual quotes\n",
      "  üìñ Enhanced documentation (12+ sources)\n",
      "  üéØ From beginner Python to advanced GPU computing\n",
      "============================================================\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://e6696233e3027eda9b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e6696233e3027eda9b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üöÄ AI Tutor - Clean Working DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import random\n",
    "import difflib\n",
    "import re\n",
    "from types import SimpleNamespace\n",
    "import ast\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Try importing plotly, fallback gracefully if not available\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Plotly not available. Install with: pip install plotly\")\n",
    "    PLOTLY_AVAILABLE = False\n",
    "    go = None\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# Includes: FAISS RAG system, fallback to Falcon-7B-Instruct, and full game mode\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "# EMBED_MODEL = \"/scratch/ntiwar12/huggingfacesentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#\"BAAI/bge-m3\"\n",
    "#\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL = \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "#\"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "#\"microsoft/phi-4\"\n",
    "# LLM_MODEL = \"microsoft/DialoGPT-small\"  # Much smaller model\n",
    "#EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# LLM_MODEL = \"MiniMaxAI/SynLogic-7B\"\n",
    "# LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    " \n",
    "#\"meta-llama/Meta-Llama-3-8B\"\n",
    "#\"allenai/digital-socrates-13b\"\n",
    "#\"/scratch/ntiwar12/huggingface/hub/models--WizardLM--WizardCoder-Python-34B-V1.0/snapshots/897fc6d9e12136c68c441b2350d015902c144b20/\"\n",
    "#\"allenai/digital-socrates-13b\"\n",
    "\n",
    "\n",
    "#\"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "#https://huggingface.co/tecosys/Nutaan-RL1\n",
    "#https://huggingface.co/knowledgator/Qwen-encoder-0.5B\n",
    "#https://huggingface.co/knowledgator/Llama-encoder-1.0B\n",
    "CHUNK_FILE = \"chunks.pkl\"\n",
    "INDEX_FILE = \"faiss.index\" \n",
    "URLS_FILE = \"custom_urls.txt\"\n",
    "\n",
    "# Enhanced documentation system\n",
    "DOC_LINKS = {\n",
    "    # Core Libraries\n",
    "    'pandas': 'https://pandas.pydata.org/docs/',\n",
    "    'scikit-learn': 'https://scikit-learn.org/stable/',\n",
    "    'rapids': 'https://rapids.ai/',\n",
    "    'cudf': 'https://docs.rapids.ai/api/cudf/stable/',\n",
    "    'cupy': 'https://docs.cupy.dev/en/stable/',\n",
    "    'pytorch': 'https://pytorch.org/docs/stable/',\n",
    "    'tensorflow': 'https://www.tensorflow.org/',\n",
    "    'cuml': 'https://docs.rapids.ai/api/cuml/stable/',\n",
    "    \n",
    "    # Data Science\n",
    "    'dsml_pdf': 'https://people.smp.uq.edu.au/DirkKroese/DSML/DSML.pdf',\n",
    "    'awesome_datascience': 'https://github.com/academic/awesome-datascience',\n",
    "    'gfg_ds_beginners': 'https://www.geeksforgeeks.org/data-science/data-science-for-beginners/',\n",
    "    \n",
    "    # GPU Acceleration\n",
    "    'cudf_pandas': 'https://rapids.ai/cudf-pandas/',\n",
    "    'polars_gpu': 'https://rapids.ai/polars-gpu-engine/',\n",
    "    'cuml_accel': 'https://rapids.ai/cuml-accel/',\n",
    "    'nx_cugraph': 'https://rapids.ai/nx-cugraph/',\n",
    "    'hagedorn2022': 'https://proceedings.mlr.press/v185/hagedorn22a/hagedorn22a.pdf',\n",
    "    'nvidia_tesla_whitepaper': 'https://www.nvidia.com/docs/io/116711/sc11-nv-tesla.pdf',\n",
    "    'cuda_blog': 'https://developer.nvidia.com/blog/tag/cuda/',\n",
    "    'libcxxgpu_pdf': 'https://www.seas.upenn.edu/~delozier/docs/libcxxgpu.pdf',\n",
    "    'gpu_accel_slides': 'https://dlsyscourse.org/slides/12-gpu-acceleration.pdf',\n",
    "    'gpucad_paper': 'https://yibolin.com/publications/papers/GPUCAD_ICCAD2020_Lin.pdf',\n",
    "    'gpucad_slides': 'https://yibolin.com/publications/papers/GPUCAD_ICCAD2020_Lin.slides.pdf',\n",
    "    'spie_gpu_sample': 'https://www.spiedigitallibrary.org/samples/SL34.pdf',\n",
    "    'gpu_to_web': 'https://www.khronos.org/assets/uploads/developers/library/2012-the-graphical-web/GPU-to-the-web_Sep2012.pdf',\n",
    "    \n",
    "    # RAG Retrieval\n",
    "    'langgraph_agentic_rag': 'https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/',\n",
    "    \n",
    "    # Basics\n",
    "    'gpu_compare': 'https://www.nvidia.com/en-us/geforce/graphics-cards/compare/',\n",
    "    'gpu_hierarchy': 'https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device='cuda')\n",
    "\n",
    "print(f\"ü§ñ Loading {LLM_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    trust_remote_code=True, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ") \n",
    "\n",
    "def stream_llm_response(prompt, max_new_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate streaming response from LLM\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Initialize generation parameters\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "                \n",
    "                # Get the new token\n",
    "                new_token_id = outputs.sequences[0][-1].item()\n",
    "                \n",
    "                # Decode the new token\n",
    "                new_token = tokenizer.decode([new_token_id], skip_special_tokens=True)\n",
    "                \n",
    "                # Check for end of sequence\n",
    "                if new_token_id == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Update inputs for next generation\n",
    "                input_ids = outputs.sequences\n",
    "                attention_mask = torch.cat([\n",
    "                    attention_mask, \n",
    "                    torch.ones((1, 1), device=model.device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                yield new_token\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Streaming generation failed: {e}\")\n",
    "        yield \"Sorry, I encountered an error while generating the response.\"\n",
    "\n",
    "print(\"‚úÖ LLM loaded and ready!\")\n",
    "\n",
    "# === RAG UTILITIES ===\n",
    "\n",
    "def read_urls_from_txt(path):\n",
    "    \"\"\"Read URLs from a text file\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è URLs file not found: {path}. Creating default URLs...\")\n",
    "        create_default_urls_file(path)\n",
    "        return read_urls_from_txt(path)\n",
    "\n",
    "def create_default_urls_file(path):\n",
    "    \"\"\"Create a default URLs file with common data science resources\"\"\"\n",
    "    default_urls = [\n",
    "        \"https://pandas.pydata.org/docs/\",\n",
    "        \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "        \"https://rapids.ai/\",\n",
    "        \"https://scikit-learn.org/stable/\",\n",
    "        \"https://pytorch.org/docs/stable/\",\n",
    "        \"https://www.tensorflow.org/\",\n",
    "        \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "        \"https://people.smp.uq.edu.au/DirkKroese/DSML/DSML.pdf\"\n",
    "    ]\n",
    "    with open(path, \"w\") as f:\n",
    "        for url in default_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "    print(f\"‚úÖ Created default URLs file: {path}\")\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    \"\"\"Fetch and clean text content from a URL\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (RAG Tutor Bot)'}\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        \n",
    "        # Filter out non-text content types\n",
    "        content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"text/\") and \"html\" not in content_type:\n",
    "            print(f\"‚ö†Ô∏è Skipping non-text URL: {url} (type={content_type})\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Parse HTML content safely\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(separator=\"\\n\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "def load_all_chunks(urls):\n",
    "    \"\"\"Load and chunk text from all URLs\"\"\"\n",
    "    all_chunks = []\n",
    "    for url in urls:\n",
    "        print(f\"üì• Processing: {url}\")\n",
    "        txt = fetch_text_from_url(url)\n",
    "        if txt:\n",
    "            chunks = chunk_text(txt)\n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"‚úÖ Added {len(chunks)} chunks from {url}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No content from {url}\")\n",
    "    return all_chunks\n",
    "\n",
    "def embed_and_index(chunks):\n",
    "    \"\"\"Create embeddings and FAISS index\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è No chunks to embed\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîÑ Creating embeddings for {len(chunks)} chunks...\")\n",
    "    vecs = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vecs))\n",
    "    print(f\"‚úÖ Created FAISS index with {len(chunks)} chunks\")\n",
    "    return index\n",
    "\n",
    "def query_rag(query, index, chunks, k=3):\n",
    "    \"\"\"Query the RAG system\"\"\"\n",
    "    if index is None or not chunks:\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    q_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(np.array(q_vec), k)\n",
    "    retrieved_chunks = [chunks[i] for i in I[0] if i < len(chunks)]\n",
    "    return \"\\n\".join(retrieved_chunks), \"Custom Knowledge Base\"\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cached chunks and index\"\"\"\n",
    "    if os.path.exists(CHUNK_FILE) and os.path.exists(INDEX_FILE):\n",
    "        try:\n",
    "            with open(CHUNK_FILE, \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "            index = faiss.read_index(INDEX_FILE)\n",
    "            print(f\"‚úÖ Loaded cached RAG with {len(chunks)} chunks\")\n",
    "            return chunks, index\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading cache: {e}\")\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "def save_cache(chunks, index):\n",
    "    \"\"\"Save chunks and index to cache\"\"\"\n",
    "    try:\n",
    "        with open(CHUNK_FILE, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        faiss.write_index(index, INDEX_FILE)\n",
    "        print(f\"‚úÖ Cached RAG with {len(chunks)} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving cache: {e}\")\n",
    "\n",
    "def build_or_load_rag():\n",
    "    \"\"\"Build or load RAG system\"\"\"\n",
    "    print(\"üîÑ Initializing RAG system...\")\n",
    "    chunks, index = load_cache()\n",
    "    \n",
    "    if not chunks or index is None:\n",
    "        print(\"üîÑ Building new RAG index...\")\n",
    "        urls = read_urls_from_txt(URLS_FILE)\n",
    "        chunks = load_all_chunks(urls)\n",
    "        \n",
    "        if chunks:\n",
    "            index = embed_and_index(chunks)\n",
    "            save_cache(chunks, index)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No chunks loaded, RAG will be disabled\")\n",
    "            return [], None\n",
    "    \n",
    "    return chunks, index\n",
    "\n",
    "def is_garbage(text):\n",
    "    \"\"\"Check if text is low quality or corrupted\"\"\"\n",
    "    if not text or len(text) < 80:\n",
    "        return True\n",
    "    non_ascii = sum(1 for c in text if ord(c) > 126 or ord(c) < 9)\n",
    "    return non_ascii / len(text) > 0.2\n",
    "\n",
    "def suggest_related(query):\n",
    "    \"\"\"Suggest related questions based on query\"\"\"\n",
    "    related_examples = {\n",
    "        \"gpu\": [\"What is CUDA?\", \"cuDF vs pandas?\", \"cuML for ML tasks?\"],\n",
    "        \"pandas\": [\"How to groupby in pandas?\", \"pandas vs cuDF?\", \"Time series in pandas\"],\n",
    "        \"scikit-learn\": [\"What is train_test_split?\", \"GridSearchCV usage\", \"PCA in sklearn\"],\n",
    "        \"rapids\": [\"What is RAPIDS?\", \"cuDF advantages?\", \"RAPIDS ecosystem?\"],\n",
    "        \"machine learning\": [\"What is supervised learning?\", \"Cross-validation?\", \"Feature engineering?\"]\n",
    "    }\n",
    "    \n",
    "    suggestions = []\n",
    "    for keyword, rel_list in related_examples.items():\n",
    "        if re.search(rf\"\\b{keyword}\\b\", query, re.I):\n",
    "            suggestions = rel_list\n",
    "            break\n",
    "    \n",
    "    if suggestions:\n",
    "        bullets = \"\".join([f\"- {q}<br>\" for q in suggestions])\n",
    "        return f\"<br><br><b>üîé You might also ask:</b><br>{bullets}\"\n",
    "    return \"\"\n",
    "\n",
    "def generate_example_plot():\n",
    "    \"\"\"Generate example CPU vs GPU performance plot\"\"\"\n",
    "    if not PLOTLY_AVAILABLE:\n",
    "        return \"<p><i>üìä Chart visualization requires plotly. Install with: <code>pip install plotly</code></i></p>\"\n",
    "    \n",
    "    try:\n",
    "        x = [100, 500, 1000, 5000, 10000]\n",
    "        cpu_times = [0.5, 1.8, 3.2, 15.0, 30.0]\n",
    "        gpu_times = [0.2, 0.5, 0.8, 2.5, 5.0]\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=x, y=cpu_times, mode='lines+markers', name='CPU'))\n",
    "        fig.add_trace(go.Scatter(x=x, y=gpu_times, mode='lines+markers', name='GPU'))\n",
    "        fig.update_layout(\n",
    "            title='CPU vs GPU Execution Time',\n",
    "            xaxis_title='Input Size',\n",
    "            yaxis_title='Time (s)',\n",
    "            legend_title='Processor'\n",
    "        )\n",
    "\n",
    "        buffer = BytesIO()\n",
    "        fig.write_image(buffer, format='png')\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "        return f'<img src=\"data:image/png;base64,{img_base64}\" style=\"width:100%;max-width:600px;\">'\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Plot generation failed: {e}\")\n",
    "        return \"<p><i>üìä Chart generation temporarily unavailable</i></p>\"\n",
    "\n",
    "#from transformers import pipeline\n",
    "\n",
    "# Example setup (adjust as per your model and tokenizer setup)\n",
    "#llm_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-2\", device=0)\n",
    "\n",
    "def polish_response(text):\n",
    "    \"\"\"Polish RAG response using LLM with streaming\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You're an expert Socratic AI tutor for Data Science and GPU Acceleration. Take the content below and improve it into a well-structured explanation that is:\n",
    "\n",
    "- Friendly and clear\n",
    "- Easy for students to understand\n",
    "- Broken into logical sections. Ask questions before you answer. Make them think.\n",
    "- Uses analogies/examples when helpful\n",
    "- Suggests any charts or visualizations if relevant\n",
    "- Includes a short summary or takeaway at the end\n",
    "\n",
    "Original content:\n",
    "-------------------------------\n",
    "{text}\n",
    "-------------------------------\n",
    "\n",
    "Now improve and rewrite it in tutor style:\n",
    "\"\"\"\n",
    "    try:\n",
    "        # Use streaming for polish_response\n",
    "        response_parts = []\n",
    "        for token in stream_llm_response(prompt, max_new_tokens=1500, temperature=0.7):\n",
    "            response_parts.append(token)\n",
    "        \n",
    "        result = \"\".join(response_parts)\n",
    "        # Extract the actual response after the prompt\n",
    "        split = result.split(\"Now improve and rewrite it in tutor style:\")\n",
    "        return split[-1].strip() if len(split) > 1 else result.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå polish_response failed: {e}\")\n",
    "        return \"Sorry, I couldn't process this content right now.\"\n",
    "        \n",
    "def generate_flashcards_from_rag(query, k=3):\n",
    "    \"\"\"Generate flashcards using RAG context\"\"\"\n",
    "    if rag_index is not None and rag_chunks:\n",
    "        context, source = query_rag(query, rag_index, rag_chunks)\n",
    "        \n",
    "        if context and len(context.strip()) > 100:\n",
    "            prompt = f\"\"\"\n",
    "You're an expert AI tutor for Data Science and GPU Acceleration. From the text below, extract {k} flashcards. Each flashcard should have:\n",
    "- A question on the front (about a key concept)\n",
    "- A short, accurate answer on the back\n",
    "\n",
    "Text:\n",
    "--------------------\n",
    "{context}\n",
    "--------------------\n",
    "\n",
    "Now generate {k} flashcards in this JSON format:\n",
    "[\n",
    "  {{\"front\": \"What is ...?\", \"back\": \"It is ...\"}},\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "            try:\n",
    "                raw = llm_pipeline(prompt, max_new_tokens=200, temperature=0.7)[0]['generated_text']\n",
    "                match = re.search(r\"\\[(\\s*\\{.*?\\}\\s*,?\\s*)+\\]\", raw, re.DOTALL)\n",
    "                if match:\n",
    "                    return ast.literal_eval(match.group(0))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå RAG Flashcard generation failed: {e}\")\n",
    "    \n",
    "    # Fallback to original method\n",
    "    return generate_flashcards(query, k)\n",
    "\n",
    "# Initialize RAG system\n",
    "print(\"üöÄ Initializing RAG system...\")\n",
    "rag_chunks, rag_index = build_or_load_rag()\n",
    "\n",
    "# === CORE FUNCTIONS ===\n",
    "\n",
    "def generate_contextual_quote(context=\"learning\", user_query=\"\"):\n",
    "    \"\"\"Generate contextual inspirational quotes\"\"\"\n",
    "    \n",
    "    # Try LLM generation first\n",
    "    if llm_pipeline:\n",
    "        try:\n",
    "            topic = extract_topic_from_query(user_query) if user_query else context\n",
    "            prompt = f\"Generate an inspiring quote about {topic} and learning:\"\n",
    "            result = llm_pipeline(prompt, max_new_tokens=50, temperature=0.8)\n",
    "            if result and len(result) > 0:\n",
    "                quote = result[0]['generated_text'].strip()\n",
    "                if len(quote) > 10:\n",
    "                    return f\"üß† '{quote}' - AI Generated\"\n",
    "        except Exception as e:\n",
    "            print(f\"Quote generation error: {e}\")\n",
    "    \n",
    "    # Enhanced fallback quotes\n",
    "    quotes = [\n",
    "        \"üí° 'Data is the new oil, but insights are the refined fuel.' - Anonymous\",\n",
    "        \"üöÄ 'The best way to get started is to quit talking and begin doing.' - Walt Disney\",\n",
    "        \"üß† 'Machine learning is the last invention that humanity will ever need to make.' - Nick Bostrom\",\n",
    "        \"‚ö° 'GPU acceleration: Think parallel, compute faster!' - AI Generated\",\n",
    "        \"üìä 'In data we trust, but insights we must discover!' - AI Generated\",\n",
    "        \"üî¨ 'Every algorithm learns from data, just like we learn from experience!' - AI Generated\",\n",
    "        \"üéØ 'The goal is to turn data into information, and information into insight.' - Carly Fiorina\",\n",
    "        \"üåü 'Artificial intelligence is the new electricity.' - Andrew Ng\",\n",
    "        \"‚öôÔ∏è 'The key to artificial intelligence has always been the representation.' - Jeff Hawkins\"\n",
    "    ]\n",
    "    return random.choice(quotes)\n",
    "\n",
    "def extract_topic_from_query(query):\n",
    "    \"\"\"Extract main topic from user query\"\"\"\n",
    "    if not query:\n",
    "        return \"learning\"\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    topic_keywords = {\n",
    "        'gpu computing': ['gpu', 'cuda', 'parallel', 'graphics'],\n",
    "        'data science': ['data', 'science', 'analytics', 'statistics'],\n",
    "        'machine learning': ['machine learning', 'ml', 'ai', 'artificial intelligence'],\n",
    "        'pandas': ['pandas', 'dataframe'],\n",
    "        'rapids': ['rapids', 'cudf', 'cuml'],\n",
    "        'programming': ['python', 'code', 'programming']\n",
    "    }\n",
    "    \n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        if any(keyword in query_lower for keyword in keywords):\n",
    "            return topic\n",
    "    return 'learning'\n",
    "\n",
    "def smart_tutor_answer_streaming(query):\n",
    "    \"\"\"Enhanced RAG-powered Socratic tutor responses with streaming\"\"\"\n",
    "    if not query or query.strip() == \"\":\n",
    "        yield \"ü§î Please ask me a question about data science, machine learning, or GPU computing!\"\n",
    "        return\n",
    "    \n",
    "    # Try RAG first\n",
    "    if rag_index is not None and rag_chunks:\n",
    "        context, source = query_rag(query, rag_index, rag_chunks)\n",
    "        \n",
    "        if context and not is_garbage(context):\n",
    "            # Stream the polished response\n",
    "            polished_prompt = f\"\"\"\n",
    "You're a semi-Socratic AI tutor for Data Science and GPU Acceleration. Take the content below and improve it into a well-structured explanation that is:\n",
    "\n",
    "- Friendly and clear\n",
    "- Easy for students to understand\n",
    "- Broken into small logical sections. \n",
    "- Ask questions before you answer. Make them think.\n",
    "- Uses analogies/examples when helpful\n",
    "- Suggests any charts or visualizations if relevant\n",
    "- Includes a short summary or takeaway at the end\n",
    "\n",
    "Original content:\n",
    "-------------------------------\n",
    "{context.strip()}\n",
    "-------------------------------\n",
    "\n",
    "Now improve and rewrite it in tutor style:\n",
    "\"\"\"\n",
    "            for token in stream_llm_response(polished_prompt, max_new_tokens=1500, temperature=0.7):\n",
    "                yield token\n",
    "            return\n",
    "    \n",
    "    # Fallback to LLM streaming for non-RAG responses\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Generic response with LLM streaming\n",
    "    if llm_pipeline:\n",
    "        try:\n",
    "            prompt = f\"\"\"You are a thoughtful, semi-socratic, and concise AI tutor specialized in data science and GPU acceleration.\n",
    "    \n",
    "    Your goal is to guide the learner with an engaging and structured explanation. \n",
    "    - Start by briefly asking a clarifying or reflective question (Socratic style)\n",
    "    - Then answer clearly, avoiding jargon. Keep answers short.\n",
    "    - Use examples or analogies if helpful\n",
    "    \n",
    "    Q: {query}\n",
    "    A:\"\"\"\n",
    "            for token in stream_llm_response(prompt, max_new_tokens=1000, temperature=0.7):\n",
    "                yield token\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå LLM fallback failed: {e}\")\n",
    "\n",
    "    yield f\"\"\"ü§î Let's explore this together:\n",
    "\n",
    "‚Ä¢ What do you already know about {query}?\n",
    "‚Ä¢ How might this relate to data science or computing?\n",
    "‚Ä¢ What would you want to accomplish by understanding this better?\n",
    "\n",
    "üí° **Hint:** Break down the concept into smaller parts and think about how each works.\"\"\"\n",
    "\n",
    "def generate_flashcards(topic, n=3):\n",
    "    \"\"\"Generate educational flashcards with RAG enhancement\"\"\"\n",
    "    \n",
    "    # Try RAG-based generation first\n",
    "    if rag_index is not None and rag_chunks:\n",
    "        try:\n",
    "            context, source = query_rag(topic, rag_index, rag_chunks)\n",
    "            if context and len(context.strip()) > 100:\n",
    "                prompt = f\"\"\"\n",
    "Create {n} educational flashcards about {topic} from this content. Format as JSON array:\n",
    "[{{\"front\": \"question\", \"back\": \"answer\"}}]\n",
    "\n",
    "Content:\n",
    "{context[:1000]}\n",
    "\"\"\"\n",
    "                result = llm_pipeline(prompt, max_new_tokens=1000, temperature=0.7)\n",
    "                if result:\n",
    "                    text = result[0]['generated_text']\n",
    "                    # Try to extract JSON array\n",
    "                    match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "                    if match:\n",
    "                        try:\n",
    "                            cards = ast.literal_eval(match.group(0))\n",
    "                            if isinstance(cards, list) and len(cards) > 0:\n",
    "                                return cards[:n]\n",
    "                        except:\n",
    "                            pass\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå RAG flashcard generation failed: {e}\")\n",
    "    \n",
    "    # Fallback flashcards\n",
    "    flashcard_pools = {\n",
    "        \"cudf\": [\n",
    "            {\"front\": \"What is cuDF?\", \"back\": \"cuDF is a GPU DataFrame library with pandas-like API.\"},\n",
    "            {\"front\": \"How to convert pandas to cuDF?\", \"back\": \"Use cudf.from_pandas(df)\"},\n",
    "            {\"front\": \"Main advantage of cuDF?\", \"back\": \"GPU acceleration for large dataset processing\"}\n",
    "        ],\n",
    "        \"pandas\": [\n",
    "            {\"front\": \"What is a DataFrame?\", \"back\": \"A 2D labeled data structure with columns of different types.\"},\n",
    "            {\"front\": \"How to read CSV in pandas?\", \"back\": \"pd.read_csv('filename.csv')\"},\n",
    "            {\"front\": \"How to select a column?\", \"back\": \"df['column_name'] or df.column_name\"}\n",
    "        ],\n",
    "        \"rapids\": [\n",
    "            {\"front\": \"What is RAPIDS?\", \"back\": \"A suite of GPU-accelerated data science libraries.\"},\n",
    "            {\"front\": \"Main RAPIDS libraries?\", \"back\": \"cuDF, cuML, cuGraph for DataFrames, ML, and graphs.\"},\n",
    "            {\"front\": \"RAPIDS vs traditional tools?\", \"back\": \"Same APIs but with GPU acceleration for massive speedups.\"}\n",
    "        ],\n",
    "        \"cuda\": [\n",
    "            {\"front\": \"What does CUDA stand for?\", \"back\": \"Compute Unified Device Architecture\"},\n",
    "            {\"front\": \"What is CUDA used for?\", \"back\": \"Parallel computing on NVIDIA GPUs\"},\n",
    "            {\"front\": \"CUDA vs CPU computing?\", \"back\": \"CUDA enables massive parallelization vs sequential CPU processing\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Get cards for topic, with fallback\n",
    "    cards = flashcard_pools.get(topic.lower(), [\n",
    "        {\"front\": f\"What is {topic}?\", \"back\": f\"{topic} is an important concept in data science.\"},\n",
    "        {\"front\": f\"Why learn {topic}?\", \"back\": f\"Understanding {topic} improves your data science skills.\"}\n",
    "    ])\n",
    "    \n",
    "    return cards[:n]\n",
    "\n",
    "def generate_quiz(topic, n=2):\n",
    "    \"\"\"Generate quiz questions\"\"\"\n",
    "    \n",
    "    # Quiz pools by topic\n",
    "    quiz_pools = {\n",
    "        \"pandas\": [\n",
    "            {\"question\": \"What is the main data structure in pandas?\", \"options\": [\"DataFrame\", \"Array\", \"List\", \"Dict\"], \"answer_idx\": 0},\n",
    "            {\"question\": \"How do you read a CSV file?\", \"options\": [\"pd.read_csv()\", \"pd.load()\", \"pd.import()\", \"pd.open()\"], \"answer_idx\": 0}\n",
    "        ],\n",
    "        \"cudf\": [\n",
    "            {\"question\": \"cuDF accelerates which library?\", \"options\": [\"NumPy\", \"Pandas\", \"SciPy\", \"Matplotlib\"], \"answer_idx\": 1},\n",
    "            {\"question\": \"cuDF runs on which hardware?\", \"options\": [\"CPU\", \"GPU\", \"TPU\", \"FPGA\"], \"answer_idx\": 1}\n",
    "        ],\n",
    "        \"rapids\": [\n",
    "            {\"question\": \"RAPIDS is developed by?\", \"options\": [\"Google\", \"Facebook\", \"NVIDIA\", \"Microsoft\"], \"answer_idx\": 2},\n",
    "            {\"question\": \"Main RAPIDS component for DataFrames?\", \"options\": [\"cuML\", \"cuDF\", \"cuGraph\", \"cuPy\"], \"answer_idx\": 1}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    questions = quiz_pools.get(topic.lower(), [\n",
    "        {\"question\": f\"What is {topic}?\", \"options\": [\"A tool\", \"A library\", \"A concept\", \"All of above\"], \"answer_idx\": 3},\n",
    "        {\"question\": f\"Why is {topic} important?\", \"options\": [\"Performance\", \"Efficiency\", \"Scale\", \"All of above\"], \"answer_idx\": 3}\n",
    "    ])\n",
    "    \n",
    "    return random.sample(questions, min(n, len(questions)))\n",
    "\n",
    "def generate_coding_puzzle(difficulty=\"Beginner\"):\n",
    "    \"\"\"Generate coding puzzles for pandas to cuDF conversion\"\"\"\n",
    "    \n",
    "    puzzles = {\n",
    "        \"Beginner\": {\n",
    "            \"description\": \"Convert basic DataFrame creation from pandas to cuDF\",\n",
    "            \"cpu_code\": \"import pandas as pd\\ndf = pd.DataFrame({'a': [1, 2, 3]})\\nprint(df)\",\n",
    "            \"gpu_code\": \"import cudf\\ndf = cudf.DataFrame({'a': [1, 2, 3]})\\nprint(df)\",\n",
    "            \"cpu_time\": 1.2,\n",
    "            \"gpu_time\": 0.2\n",
    "        },\n",
    "        \"Intermediate\": {\n",
    "            \"description\": \"Convert groupby operation from pandas to cuDF\",\n",
    "            \"cpu_code\": \"import pandas as pd\\ndf = pd.DataFrame({'group': ['A', 'B', 'A'], 'value': [1, 2, 3]})\\nresult = df.groupby('group').sum()\",\n",
    "            \"gpu_code\": \"import cudf\\ndf = cudf.DataFrame({'group': ['A', 'B', 'A'], 'value': [1, 2, 3]})\\nresult = df.groupby('group').sum()\",\n",
    "            \"cpu_time\": 3.2,\n",
    "            \"gpu_time\": 0.5\n",
    "        },\n",
    "        \"Advanced\": {\n",
    "            \"description\": \"Convert complex aggregation from pandas to cuDF\",\n",
    "            \"cpu_code\": \"import pandas as pd\\ndf = pd.DataFrame({'cat': ['A', 'B'], 'val1': [1, 2], 'val2': [3, 4]})\\nresult = df.groupby('cat').agg({'val1': 'sum', 'val2': 'mean'})\",\n",
    "            \"gpu_code\": \"import cudf\\ndf = cudf.DataFrame({'cat': ['A', 'B'], 'val1': [1, 2], 'val2': [3, 4]})\\nresult = df.groupby('cat').agg({'val1': 'sum', 'val2': 'mean'})\",\n",
    "            \"cpu_time\": 4.2,\n",
    "            \"gpu_time\": 0.6\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return puzzles.get(difficulty, puzzles[\"Beginner\"])\n",
    "\n",
    "# === GRADIO INTERFACE ===\n",
    "def create_ai_tutor():\n",
    "    \"\"\"Create the AI Tutor Gradio interface\"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"üß† AI Tutor\") as app:\n",
    "        \n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div style='text-align:center; background:linear-gradient(90deg,#fff1c1,#c1e7ff,#e1ffc1); border-radius:15px; padding:15px; margin-bottom:20px; color:#000;'>\n",
    "          <h1 style='color:#000; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);'>üß† AI Tutor - Complete Learning System</h1>\n",
    "          <h3 style='color:#000; font-weight:700; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);'>üìö Learning Mode ‚Ä¢ üéì Socratic Tutoring ‚Ä¢ üéÆ Gamified Practice</h3>\n",
    "          <p style='color:#000; font-weight:600; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);'><b>üìö Structured Learning Paths ‚Ä¢ ü§î Guided Discovery ‚Ä¢ üéØ Interactive Practice</b></p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Add custom CSS for better styling\n",
    "        app.css = \"\"\"\n",
    "        .learning-topic-btn {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;\n",
    "            color: white !important;\n",
    "            border: none !important;\n",
    "            border-radius: 12px !important;\n",
    "            padding: 15px !important;\n",
    "            margin: 8px !important;\n",
    "            font-weight: 600 !important;\n",
    "            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4) !important;\n",
    "            transition: all 0.3s ease !important;\n",
    "            min-height: 80px !important;\n",
    "            font-size: 14px !important;\n",
    "        }\n",
    "        .learning-topic-btn:hover {\n",
    "            transform: translateY(-2px) !important;\n",
    "            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6) !important;\n",
    "        }\n",
    "        \n",
    "        /* Make tab headers more prominent */\n",
    "        .gradio-tab-nav {\n",
    "            background: linear-gradient(90deg, #f8fafc, #e2e8f0) !important;\n",
    "            border-radius: 8px !important;\n",
    "            padding: 8px !important;\n",
    "            margin-bottom: 20px !important;\n",
    "            box-shadow: 0 2px 8px rgba(0,0,0,0.1) !important;\n",
    "        }\n",
    "        \n",
    "        .gradio-tab-nav .tab-nav {\n",
    "            background: linear-gradient(135deg, #4f46e5, #7c3aed) !important;\n",
    "            color: white !important;\n",
    "            font-weight: 700 !important;\n",
    "            font-size: 16px !important;\n",
    "            border-radius: 6px !important;\n",
    "            margin: 4px !important;\n",
    "            padding: 12px 20px !important;\n",
    "            box-shadow: 0 2px 4px rgba(79, 70, 229, 0.3) !important;\n",
    "            border: none !important;\n",
    "        }\n",
    "        \n",
    "        .gradio-tab-nav .tab-nav:hover {\n",
    "            background: linear-gradient(135deg, #6366f1, #8b5cf6) !important;\n",
    "            transform: translateY(-1px) !important;\n",
    "            box-shadow: 0 4px 8px rgba(79, 70, 229, 0.4) !important;\n",
    "        }\n",
    "        \n",
    "        .gradio-tab-nav .tab-nav.selected {\n",
    "            background: linear-gradient(135deg, #059669, #0d9488) !important;\n",
    "            box-shadow: 0 4px 12px rgba(5, 150, 105, 0.4) !important;\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            \n",
    "            # === LEARNING MODE ===\n",
    "            with gr.Tab(\"üìö Learning Mode\"):\n",
    "                gr.Markdown(\"### üéØ **Choose a Learning Path - From Beginner to Advanced**\")\n",
    "                gr.Markdown(\"*Select any topic below to get a structured learning journey with resources and related topics*\")\n",
    "                \n",
    "                # Learning topics with structured paths\n",
    "                LEARNING_TOPICS = {\n",
    "                    \"Python for Data Science\": {\n",
    "                        \"description\": \"Master Python fundamentals and data science libraries\",\n",
    "                        \"level\": \"üü¢ Beginner to Intermediate\",\n",
    "                        \"duration\": \"4-6 weeks\",\n",
    "                        \"prerequisites\": \"Basic programming knowledge\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **Python Basics** - Variables, data types, control structures\n",
    "2. **NumPy** - Numerical computing and arrays\n",
    "3. **Pandas** - Data manipulation and analysis\n",
    "4. **Matplotlib/Seaborn** - Data visualization\n",
    "5. **Jupyter Notebooks** - Interactive development\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Python.org Tutorial](https://docs.python.org/3/tutorial/)\n",
    "‚Ä¢ [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "‚Ä¢ [NumPy User Guide](https://numpy.org/doc/stable/user/)\n",
    "\n",
    "**üöÄ Next Steps:** Machine Learning Fundamentals, Data Visualization\n",
    "\"\"\",\n",
    "                        \"related\": [\"Machine Learning Fundamentals\", \"Data Visualization\", \"Pandas Deep Dive\"]\n",
    "                    },\n",
    "                    \"Pandas Deep Dive\": {\n",
    "                        \"description\": \"Master DataFrame operations, data cleaning, and advanced pandas techniques\",\n",
    "                        \"level\": \"üü° Intermediate\",\n",
    "                        \"duration\": \"3-4 weeks\", \n",
    "                        \"prerequisites\": \"Python basics, basic pandas knowledge\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **DataFrame Mastery** - Creation, indexing, selection\n",
    "2. **Data Cleaning** - Missing values, duplicates, data types\n",
    "3. **GroupBy Operations** - Aggregation and transformation\n",
    "4. **Merging & Joining** - Combining datasets\n",
    "5. **Performance Optimization** - Efficient pandas operations\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "‚Ä¢ [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)\n",
    "‚Ä¢ [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "\n",
    "**üöÄ Next Steps:** GPU Acceleration with cuDF, Time Series Analysis\n",
    "\"\"\",\n",
    "                        \"related\": [\"GPU Acceleration with cuDF\", \"Data Visualization\", \"Time Series Analysis\"]\n",
    "                    },\n",
    "                    \"GPU Acceleration with cuDF\": {\n",
    "                        \"description\": \"Learn GPU-accelerated data processing with RAPIDS cuDF\",\n",
    "                        \"level\": \"üî¥ Advanced\",\n",
    "                        \"duration\": \"2-3 weeks\",\n",
    "                        \"prerequisites\": \"Strong pandas knowledge, basic GPU concepts\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **GPU Computing Basics** - Understanding parallel processing\n",
    "2. **cuDF Introduction** - GPU DataFrames and basic operations\n",
    "3. **Migration from Pandas** - Converting existing code\n",
    "4. **Performance Optimization** - Memory management and best practices\n",
    "5. **Advanced Operations** - Complex aggregations and joins\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [RAPIDS cuDF Documentation](https://docs.rapids.ai/api/cudf/stable/)\n",
    "‚Ä¢ [cuDF User Guide](https://docs.rapids.ai/api/cudf/stable/user_guide/)\n",
    "‚Ä¢ [RAPIDS Getting Started](https://rapids.ai/start.html)\n",
    "\n",
    "**üöÄ Next Steps:** RAPIDS Ecosystem, Machine Learning with cuML\n",
    "\"\"\",\n",
    "                        \"related\": [\"RAPIDS Ecosystem\", \"CUDA Programming\", \"High-Performance Computing\"]\n",
    "                    },\n",
    "                    \"Machine Learning Fundamentals\": {\n",
    "                        \"description\": \"Core ML concepts, algorithms, and scikit-learn implementation\",\n",
    "                        \"level\": \"üü° Intermediate\",\n",
    "                        \"duration\": \"6-8 weeks\",\n",
    "                        \"prerequisites\": \"Python, pandas, basic statistics\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **ML Concepts** - Supervised, unsupervised, reinforcement learning\n",
    "2. **Data Preprocessing** - Feature scaling, encoding, train-test splits\n",
    "3. **Regression Algorithms** - Linear, polynomial, regularization\n",
    "4. **Classification** - Logistic regression, decision trees, SVM\n",
    "5. **Model Evaluation** - Cross-validation, metrics, hyperparameter tuning\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "‚Ä¢ [Machine Learning Course by Andrew Ng](https://www.coursera.org/learn/machine-learning)\n",
    "‚Ä¢ [Hands-On Machine Learning](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "**üöÄ Next Steps:** Deep Learning, GPU-Accelerated ML with cuML\n",
    "\"\"\",\n",
    "                        \"related\": [\"Deep Learning Fundamentals\", \"GPU-Accelerated ML\", \"Data Science Projects\"]\n",
    "                    },\n",
    "                    \"RAPIDS Ecosystem\": {\n",
    "                        \"description\": \"Complete RAPIDS suite: cuDF, cuML, cuGraph for end-to-end GPU data science\",\n",
    "                        \"level\": \"üî¥ Advanced\",\n",
    "                        \"duration\": \"4-5 weeks\",\n",
    "                        \"prerequisites\": \"GPU computing basics, pandas, scikit-learn\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **RAPIDS Overview** - cuDF, cuML, cuGraph, cuPy integration\n",
    "2. **cuDF Mastery** - Advanced DataFrame operations on GPU\n",
    "3. **cuML for ML** - GPU-accelerated machine learning algorithms\n",
    "4. **cuGraph** - Graph analytics and network analysis\n",
    "5. **End-to-End Workflows** - Complete GPU data science pipelines\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [RAPIDS.ai Main Site](https://rapids.ai/)\n",
    "‚Ä¢ [RAPIDS Documentation](https://docs.rapids.ai/)\n",
    "‚Ä¢ [RAPIDS Community](https://github.com/rapidsai)\n",
    "\n",
    "**üöÄ Next Steps:** Production Deployment, Multi-GPU Computing\n",
    "\"\"\",\n",
    "                        \"related\": [\"Multi-GPU Computing\", \"Production ML Systems\", \"CUDA Programming\"]\n",
    "                    },\n",
    "                    \"Data Visualization\": {\n",
    "                        \"description\": \"Create compelling data visualizations with matplotlib, seaborn, and plotly\",\n",
    "                        \"level\": \"üü¢ Beginner to Intermediate\",\n",
    "                        \"duration\": \"3-4 weeks\",\n",
    "                        \"prerequisites\": \"Python basics, pandas fundamentals\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **Matplotlib Basics** - Plots, figures, axes, customization\n",
    "2. **Seaborn for Statistics** - Statistical plots and themes\n",
    "3. **Interactive Plotly** - Dynamic and web-ready visualizations\n",
    "4. **Advanced Techniques** - Subplots, animations, custom plots\n",
    "5. **Dashboard Creation** - Streamlit, Dash for interactive apps\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "‚Ä¢ [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)\n",
    "‚Ä¢ [Plotly Python Guide](https://plotly.com/python/)\n",
    "\n",
    "**üöÄ Next Steps:** Interactive Dashboards, Business Intelligence\n",
    "\"\"\",\n",
    "                        \"related\": [\"Interactive Dashboards\", \"Business Intelligence\", \"Web Development for Data Science\"]\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Topic selection buttons\n",
    "                with gr.Row():\n",
    "                    topic_buttons = []\n",
    "                    for i, (topic_name, topic_info) in enumerate(LEARNING_TOPICS.items()):\n",
    "                        if i % 2 == 0 and i > 0:\n",
    "                            # Start new row every 2 buttons\n",
    "                            pass\n",
    "                        \n",
    "                        color_map = {\n",
    "                            \"üü¢\": \"#dcfce7\",  # Green for beginner\n",
    "                            \"üü°\": \"#fef3c7\",  # Yellow for intermediate  \n",
    "                            \"üî¥\": \"#fee2e2\"   # Red for advanced\n",
    "                        }\n",
    "                        level_color = color_map.get(topic_info[\"level\"][0], \"#f3f4f6\")\n",
    "                        \n",
    "                        btn = gr.Button(\n",
    "                            f\"{topic_name}\\n{topic_info['level']}\",\n",
    "                            elem_classes=\"learning-topic-btn\",\n",
    "                            size=\"lg\"\n",
    "                        )\n",
    "                        topic_buttons.append((btn, topic_name))\n",
    "                \n",
    "                # Learning content display\n",
    "                learning_content = gr.Markdown()\n",
    "                related_topics = gr.Markdown()\n",
    "                \n",
    "                def show_learning_content(topic_name):\n",
    "                    if topic_name not in LEARNING_TOPICS:\n",
    "                        return \"Topic not found!\", \"\"\n",
    "                    \n",
    "                    topic = LEARNING_TOPICS[topic_name]\n",
    "                    \n",
    "                    content = f\"\"\"\n",
    "# üìö {topic_name}\n",
    "\n",
    "**üìã Description:** {topic['description']}\n",
    "\n",
    "**üìä Level:** {topic['level']} | **‚è±Ô∏è Duration:** {topic['duration']} | **üìö Prerequisites:** {topic['prerequisites']}\n",
    "\n",
    "---\n",
    "\n",
    "{topic['content']}\n",
    "\n",
    "---\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    related = f\"\"\"\n",
    "### üîó **Related Learning Paths:**\n",
    "{' ‚Ä¢ '.join([f\"**{rel}**\" for rel in topic['related']])}\n",
    "\n",
    "üí° *Click any topic above to explore these related learning paths!*\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    return content, related\n",
    "                \n",
    "                # Connect buttons to content display\n",
    "                for btn, topic_name in topic_buttons:\n",
    "                    btn.click(\n",
    "                        lambda tn=topic_name: show_learning_content(tn),\n",
    "                        outputs=[learning_content, related_topics]\n",
    "                    )\n",
    "                \n",
    "                # Search functionality\n",
    "                with gr.Row():\n",
    "                    search_topic = gr.Textbox(\n",
    "                        label=\"üîç Search for specific topics\",\n",
    "                        placeholder=\"e.g., neural networks, time series, NLP, computer vision...\"\n",
    "                    )\n",
    "                    search_btn = gr.Button(\"Search Learning Resources\")\n",
    "                \n",
    "                search_results = gr.Markdown()\n",
    "                \n",
    "                def search_learning_resources(query):\n",
    "                    if not query:\n",
    "                        return \"Please enter a search term!\"\n",
    "                    \n",
    "                    # Simulate search results with relevant resources\n",
    "                    query_lower = query.lower()\n",
    "                    \n",
    "                    results = []\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['neural', 'deep', 'cnn', 'rnn', 'transformer']):\n",
    "                        results.append(\"\"\"\n",
    "**üß† Deep Learning Resources:**\n",
    "‚Ä¢ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "‚Ä¢ [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "‚Ä¢ [TensorFlow Guide](https://www.tensorflow.org/guide)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['nlp', 'text', 'language', 'sentiment']):\n",
    "                        results.append(\"\"\"\n",
    "**üìù Natural Language Processing:**\n",
    "‚Ä¢ [NLTK Documentation](https://www.nltk.org/)\n",
    "‚Ä¢ [spaCy Course](https://course.spacy.io/)\n",
    "‚Ä¢ [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['time series', 'forecasting', 'temporal']):\n",
    "                        results.append(\"\"\"\n",
    "**üìà Time Series Analysis:**\n",
    "‚Ä¢ [Time Series Analysis Guide](https://www.statsmodels.org/stable/tsa.html)\n",
    "‚Ä¢ [Prophet Forecasting](https://facebook.github.io/prophet/)\n",
    "‚Ä¢ [Time Series with Python](https://github.com/marcopeix/TimeSeriesForecastingInPython)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['computer vision', 'image', 'cv', 'opencv']):\n",
    "                        results.append(\"\"\"\n",
    "**üëÅÔ∏è Computer Vision:**\n",
    "‚Ä¢ [OpenCV Tutorials](https://docs.opencv.org/master/d9/df8/tutorial_root.html)\n",
    "‚Ä¢ [Computer Vision Course](https://www.coursera.org/learn/convolutional-neural-networks)\n",
    "‚Ä¢ [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if not results:\n",
    "                        results.append(f\"\"\"\n",
    "**üîç Search Results for \"{query}\":**\n",
    "\n",
    "*No specific resources found. Try these general resources:*\n",
    "‚Ä¢ [Kaggle Learn](https://www.kaggle.com/learn) - Free micro-courses\n",
    "‚Ä¢ [Coursera Data Science](https://www.coursera.org/browse/data-science)\n",
    "‚Ä¢ [edX MIT Data Science](https://www.edx.org/course/introduction-to-computational-thinking-and-data-science)\n",
    "\n",
    "*Or refine your search with terms like: machine learning, deep learning, NLP, computer vision, time series*\n",
    "\"\"\")\n",
    "                    \n",
    "                    return \"\\n\".join(results)\n",
    "                \n",
    "                search_btn.click(search_learning_resources, inputs=search_topic, outputs=search_results)\n",
    "\n",
    "            # === TUTOR MODE ===\n",
    "            with gr.Tab(\"üéì Tutor Mode\"):\n",
    "                gr.Markdown(\"### ü§î **Socratic Learning - Ask Questions, Get Guided Answers**\")\n",
    "                gr.Markdown(\"*I won't give you direct answers, but I'll guide you to discover the knowledge yourself!*\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        tutor_query = gr.Textbox(\n",
    "                            label=\"Ask about Data Science, GPU Computing, or type 'docs' for documentation\",\n",
    "                            placeholder=\"e.g., What is RAPIDS? How does cuDF work? Why use GPU for data science?\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        tutor_btn = gr.Button(\"üöÄ Get Socratic Guidance\", variant=\"primary\")\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        quote_display = gr.Markdown(\"üí° *Ready to discover knowledge!*\")\n",
    "                \n",
    "                tutor_answer = gr.Markdown(label=\"ü§î Socratic Guidance\")\n",
    "                tutor_source = gr.Markdown(label=\"üìö Source\")\n",
    "                \n",
    "                # Quick question suggestions\n",
    "                with gr.Row():\n",
    "                    quick_questions = [\n",
    "                        \"Why use GPUs for data science?\",\n",
    "                        \"How does parallel processing work?\", \n",
    "                        \"What makes cuDF faster than pandas?\",\n",
    "                        \"When should I use machine learning?\"\n",
    "                    ]\n",
    "                    for i, question in enumerate(quick_questions):\n",
    "                        if i % 2 == 0:\n",
    "                            with gr.Column():\n",
    "                                pass\n",
    "                        quick_btn = gr.Button(f\"üí≠ {question}\", size=\"sm\")\n",
    "                        quick_btn.click(lambda q=question: q, outputs=tutor_query)\n",
    "                \n",
    "                def handle_tutor_query_streaming(query):\n",
    "                    if not query:\n",
    "                        yield \"Please ask a question!\", \"AI Tutor\", \"üí° *Ready to help!*\"\n",
    "                        return\n",
    "                    \n",
    "                    # Handle docs command\n",
    "                    if query.lower().strip() in ['docs', 'documentation', 'help']:\n",
    "                        docs_info = \"\"\"üìö **Available Documentation:**\n",
    "‚Ä¢ **Core Libraries:** pandas, cuDF, RAPIDS, scikit-learn\n",
    "‚Ä¢ **GPU Computing:** CUDA, NVIDIA resources  \n",
    "‚Ä¢ **Data Science:** Comprehensive guides and tutorials\n",
    "*Ask any topic-specific question to get relevant documentation!*\"\"\"\n",
    "                        yield docs_info, \"Documentation System\", \"üìñ *Knowledge at your fingertips!*\"\n",
    "                        return\n",
    "                    \n",
    "                    # Generate contextual quote\n",
    "                    quote = generate_contextual_quote(extract_topic_from_query(query), query)\n",
    "                    \n",
    "                    # Stream the response\n",
    "                    accumulated_response = \"\"\n",
    "                    for token in smart_tutor_answer_streaming(query):\n",
    "                        accumulated_response += token\n",
    "                        yield accumulated_response, f\"**Source:** AI Tutor (Streaming)\", f\"üåü {quote}\"\n",
    "                \n",
    "                # Update the click handlers to use streaming\n",
    "                tutor_btn.click(\n",
    "                    handle_tutor_query_streaming, \n",
    "                    inputs=tutor_query, \n",
    "                    outputs=[tutor_answer, tutor_source, quote_display],\n",
    "                    show_progress=True\n",
    "                )\n",
    "                tutor_query.submit(\n",
    "                    handle_tutor_query_streaming, \n",
    "                    inputs=tutor_query, \n",
    "                    outputs=[tutor_answer, tutor_source, quote_display],\n",
    "                    show_progress=True\n",
    "                )\n",
    "            \n",
    "            # === GAME MODE ===\n",
    "            with gr.Tab(\"üéÆ Game Mode\"):\n",
    "                user_state = SimpleNamespace(points=0)\n",
    "                \n",
    "                def update_progress():\n",
    "                    level = user_state.points // 20 + 1\n",
    "                    bar_width = min((user_state.points % 20) * 5, 100)\n",
    "                    return f\"\"\"<h3>üèÜ Level {level} | üî• {user_state.points} XP</h3>\n",
    "                    <div style='background: #e5e7eb; height: 20px; border-radius: 10px;'>\n",
    "                        <div style='background: linear-gradient(90deg, #4ade80, #22c55e); height: 20px; width: {bar_width}%; border-radius: 10px;'></div>\n",
    "                    </div>\"\"\"\n",
    "                \n",
    "                progress = gr.HTML(update_progress())\n",
    "                \n",
    "                #with gr.Tabs():\n",
    "                    \n",
    "                # === FLASHCARDS ===\n",
    "                with gr.Tabs():\n",
    "                    with gr.Tab(\"üÉè Flashcards\"):\n",
    "                        with gr.Tab(\"üîß Generate Flashcards\"):\n",
    "                            gen_query = gr.Textbox(label=\"Enter a topic (e.g., cuDF, CUDA, RAPIDS)\", placeholder=\"cuDF vs pandas?\")\n",
    "                            gen_btn = gr.Button(\"‚ú® Generate Flashcards\")\n",
    "                \n",
    "                            card_front = gr.HTML()\n",
    "                            card_back = gr.HTML(visible=False)\n",
    "                            instruction_msg = gr.Markdown(\"üëÜ Choose a card to flip!\", visible=False)\n",
    "                            feedback_msg = gr.Markdown(visible=False)\n",
    "                \n",
    "                            with gr.Row():\n",
    "                                flip_btn1 = gr.Button(\"‚úÖ Flip Card 1\", visible=False)\n",
    "                                flip_btn2 = gr.Button(\"üîÑ Flip Card 2\", visible=False)\n",
    "                \n",
    "                            reset_btn = gr.Button(\"üîÅ Reset\", visible=False)\n",
    "                \n",
    "                            flash_idx = gr.State(0)\n",
    "                            show_answer = gr.State(False)\n",
    "                            flashcards_state = gr.State([])\n",
    "                \n",
    "                            import ast\n",
    "                            import random\n",
    "                            import re\n",
    "                            def generate_flashcards_from_rag(query, index, chunks, k=3):\n",
    "                                context, source = query_rag(query, index, chunks)\n",
    "                            \n",
    "                                if not context or len(context.strip()) < 100:\n",
    "                                    return [{\"front\": \"Could not find content\", \"back\": \"Try another topic.\", \"wrong\": \"This is a placeholder wrong answer.\"}]\n",
    "                            \n",
    "                                prompt = f\"\"\"\n",
    "                            You're an expert AI tutor. From the following technical text, generate exactly {k} multiple-choice style flashcards in **valid JSON**.\n",
    "                            \n",
    "                            Each flashcard must be a dictionary with:\n",
    "                            - \"front\": the question\n",
    "                            - \"back\": the correct answer\n",
    "                            - \"wrong\": a wrong but plausible distractor\n",
    "                            \n",
    "                            Example format:\n",
    "                            \n",
    "                            [\n",
    "                              {{\n",
    "                                \"front\": \"Why is cuDF faster than pandas?\",\n",
    "                                \"back\": \"It uses GPU acceleration.\",\n",
    "                                \"wrong\": \"Because it uses more RAM.\"\n",
    "                              }},\n",
    "                              ...\n",
    "                            ]\n",
    "                            \n",
    "                            DO NOT add explanation, markdown, or anything else. Just output a valid JSON list of 3 objects.\n",
    "                            \n",
    "                            Text:\n",
    "                            ------------------------\n",
    "                            {context}\n",
    "                            ------------------------\n",
    "                            \"\"\"\n",
    "                            \n",
    "                                try:\n",
    "                                    raw = llm_pipeline(prompt)[0]['generated_text']\n",
    "                            \n",
    "                                    # DEBUG: Log raw output when things fail\n",
    "                                    print(\"\\nüîç RAW OUTPUT FROM MODEL:\\n\", raw[:500], \"...\\n\")\n",
    "                            \n",
    "                                    json_text = re.search(r'\\[\\s*{.*?}\\s*\\]', raw, re.DOTALL)\n",
    "                                    if json_text:\n",
    "                                        return ast.literal_eval(json_text.group())\n",
    "                            \n",
    "                                except Exception as e:\n",
    "                                    print(f\"‚ùå Flashcard generation failed: {e}\")\n",
    "                            \n",
    "                                # fallback\n",
    "                                return [{\"front\": \"Could not parse flashcards\", \"back\": \"\", \"wrong\": \"\"}]\n",
    "                            def style_card(text):\n",
    "                                return f\"\"\"\n",
    "                <div style='font-size:18px;padding:10px;border:2px solid #ccc;border-radius:10px;background:#fefefe;transition:transform 0.5s;transform-style: preserve-3d;'>\n",
    "                {text}\n",
    "                </div>\"\"\"\n",
    "                \n",
    "                            def flashcard_from_query(topic, idx):\n",
    "                                if not topic or topic.strip() == \"\":\n",
    "                                    return (\n",
    "                                            style_card(\"**Q:** Please enter a topic to generate flashcards.\"),\n",
    "                                            \"\",\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=True, value=\"<span style='color:red; font-size:18px;'>‚ùó Please enter a topic to begin.</span>\"),\n",
    "                                            idx, False, [], True, False ) # empty flashcard state\n",
    "                                cards = generate_flashcards_from_rag(topic, rag_index, rag_chunks)\n",
    "                                #cards = generate_flashcards_from_rag(topic)\n",
    "                                idx = (idx + 1) % len(cards)\n",
    "                                card = cards[idx]\n",
    "                                buttons = [(\"‚úÖ Flip Card üßê \", True), (\"‚úÖ Flip Card ü§ì\", False)]\n",
    "                                random.shuffle(buttons)\n",
    "                                return (\n",
    "                                    style_card(f\"**Q:** {card['front']}\"),\n",
    "                                    \"\",\n",
    "                                    gr.update(visible=True, value=buttons[0][0]),\n",
    "                                    gr.update(visible=True, value=buttons[1][0]),\n",
    "                                    gr.update(visible=True),\n",
    "                                    gr.update(visible=False, value=\"\"),\n",
    "                                    gr.update(visible=True, value=\"<span style='color:green; font-size:18px;'>üëÜ Choose a card to flip!</span>\"),\n",
    "                                    idx, False, cards, buttons[0][1], buttons[1][1]\n",
    "                                )\n",
    "                \n",
    "                            def flip_card(idx, cards, correct):\n",
    "                                card = cards[idx % len(cards)]\n",
    "                                front_html = style_card(f\"**Q:** {card['front']}\")\n",
    "    \n",
    "                                if correct:\n",
    "                                    back_html = style_card(f\"‚úÖ Answer: {card['back']}\")\n",
    "                                else:\n",
    "                                    back_html = gr.update(visible=False)  # hide back card for wrong pick\n",
    "                                wrong_msgs = [\n",
    "                                    \"üö´ Not quite! As they say, 'Data is the new oil'‚Äîbut you've hit a dry well. Try the other card!\",\n",
    "                                    \"üí° 'In God we trust, all others bring data.' Sadly, this card didn't bring it. Flip the other one!\",\n",
    "                                    \"üß† Oops! ‚ÄòThe greatest value of a picture is when it forces us to notice what we never expected to see.‚Äô You missed it‚Äîcheck the other card.\",\n",
    "                                    \"‚öôÔ∏è 'GPU acceleration turns hours into seconds' ‚Äî but this pick cost you a moment. Try the other one!\",\n",
    "                                    \"üìä 'Torture the data long enough, and it will confess to anything.' This card stayed silent. Flip the other!\",\n",
    "                                    \"‚ùå Wrong Pick! 'Without data, you're just another person with an opinion.' Try the other card for some real answers!\",\n",
    "                                    \"üöÄ 'GPUs don‚Äôt guess‚Äîthey compute at scale.' Your guess here missed. Go try the other card!\",\n",
    "                                    \"üîÑ Oops! You‚Äôve hit a cold cache. Try the other card for a GPU-hot answer.\",\n",
    "                                    \"üí≠ 'Data science is the art of turning data into insight'‚Äîbut this card had none. Flip the other one!\"\n",
    "                                ]\n",
    "                                msg = \"‚úÖ Nailed it! Great job.\" if correct else random.choice(wrong_msgs)\n",
    "                            \n",
    "                                return (\n",
    "                                    front_html,\n",
    "                                    back_html,\n",
    "                                    gr.update(visible=True),\n",
    "                                    gr.update(visible=True, value=f\"<span style='font-size:18px; color:#b00020;'>{msg}</span>\"),      # ‚úÖ THIS is the fix\n",
    "                                    gr.update(visible=False),                # hide instruction\n",
    "                                    idx,\n",
    "                                    True,\n",
    "                                    cards\n",
    "                                )            \n",
    "                            btn1_correct = gr.State(True)\n",
    "                            btn2_correct = gr.State(False)\n",
    "                \n",
    "                            gen_btn.click(\n",
    "                                flashcard_from_query,\n",
    "                                inputs=[gen_query, flash_idx],\n",
    "                                outputs=[card_front, card_back, flip_btn1, flip_btn2, reset_btn, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state, btn1_correct, btn2_correct]\n",
    "                            )\n",
    "                \n",
    "                            flip_btn1.click(\n",
    "                                flip_card,\n",
    "                                inputs=[flash_idx, flashcards_state, btn1_correct],\n",
    "                                outputs=[card_front, card_back, card_back, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state]\n",
    "                            )\n",
    "                \n",
    "                            flip_btn2.click(\n",
    "                                flip_card,\n",
    "                                inputs=[flash_idx, flashcards_state, btn2_correct],\n",
    "                                outputs=[card_front, card_back, card_back, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state]\n",
    "                            )\n",
    "                \n",
    "                            reset_btn.click(\n",
    "                                flashcard_from_query,\n",
    "                                inputs=[gen_query, flash_idx],\n",
    "                                outputs=[card_front, card_back, flip_btn1, flip_btn2, reset_btn, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state, btn1_correct, btn2_correct]\n",
    "                            )\n",
    "                        \n",
    "                    # === QUIZ ===\n",
    "                    with gr.Tab(\"‚ùì Quiz\"):\n",
    "                        quiz_topic = gr.Textbox(label=\"Quiz Topic\", placeholder=\"e.g., pandas, cuDF, RAPIDS\")\n",
    "                        gen_quiz_btn = gr.Button(\"üß† Generate Quiz\")\n",
    "                        \n",
    "                        quiz_q1 = gr.Markdown()\n",
    "                        quiz_r1 = gr.Radio(choices=[], label=\"Answer:\", visible=False)\n",
    "                        quiz_q2 = gr.Markdown()\n",
    "                        quiz_r2 = gr.Radio(choices=[], label=\"Answer:\", visible=False)\n",
    "                        \n",
    "                        submit_btn = gr.Button(\"Submit Quiz\", visible=False)\n",
    "                        quiz_result = gr.Markdown()\n",
    "                        quiz_state = gr.State([])\n",
    "                        \n",
    "                        def generate_quiz_questions(topic):\n",
    "                            if not topic:\n",
    "                                return \"Enter a topic!\", gr.update(visible=False), \"\", gr.update(visible=False), gr.update(visible=False), []\n",
    "                            \n",
    "                            questions = generate_quiz(topic, 2)\n",
    "                            return (\n",
    "                                f\"**Q1:** {questions[0]['question']}\",\n",
    "                                gr.update(choices=questions[0]['options'], visible=True, value=None),\n",
    "                                f\"**Q2:** {questions[1]['question']}\",\n",
    "                                gr.update(choices=questions[1]['options'], visible=True, value=None),\n",
    "                                gr.update(visible=True),\n",
    "                                questions\n",
    "                            )\n",
    "                        \n",
    "                        def evaluate_quiz(ans1, ans2, questions):\n",
    "                            if not questions:\n",
    "                                return \"Quiz not loaded!\"\n",
    "                            \n",
    "                            correct = 0\n",
    "                            result_parts = []\n",
    "                            \n",
    "                            # Check Q1\n",
    "                            correct_ans1 = questions[0]['options'][questions[0]['answer_idx']]\n",
    "                            if ans1 and ans1 == correct_ans1:\n",
    "                                correct += 1\n",
    "                                result_parts.append(f\"‚úÖ **Q1:** Correct! ({ans1})\")\n",
    "                            else:\n",
    "                                result_parts.append(f\"‚ùå **Q1:** Wrong. You answered: {ans1 or 'None'}\")\n",
    "                                result_parts.append(f\"    üí° **Correct answer:** {correct_ans1}\")\n",
    "                            \n",
    "                            # Check Q2  \n",
    "                            correct_ans2 = questions[1]['options'][questions[1]['answer_idx']]\n",
    "                            if ans2 and ans2 == correct_ans2:\n",
    "                                correct += 1\n",
    "                                result_parts.append(f\"‚úÖ **Q2:** Correct! ({ans2})\")\n",
    "                            else:\n",
    "                                result_parts.append(f\"‚ùå **Q2:** Wrong. You answered: {ans2 or 'None'}\")\n",
    "                                result_parts.append(f\"    üí° **Correct answer:** {correct_ans2}\")\n",
    "                            \n",
    "                            user_state.points += correct * 5\n",
    "                            \n",
    "                            # Overall result\n",
    "                            score_emoji = \"üéâ\" if correct == 2 else \"üëç\" if correct == 1 else \"üìö\"\n",
    "                            result_parts.insert(0, f\"{score_emoji} **Final Score: {correct}/2 correct! +{correct*5} XP**\")\n",
    "                            result_parts.append(f\"\\nüèÜ **Total XP:** {user_state.points}\")\n",
    "                            \n",
    "                            return \"\\n\".join(result_parts)\n",
    "                        \n",
    "                        gen_quiz_btn.click(generate_quiz_questions, inputs=quiz_topic,\n",
    "                                         outputs=[quiz_q1, quiz_r1, quiz_q2, quiz_r2, submit_btn, quiz_state])\n",
    "                        submit_btn.click(evaluate_quiz, inputs=[quiz_r1, quiz_r2, quiz_state], outputs=quiz_result)\n",
    "                    \n",
    "                    # === CODING PUZZLES ===\n",
    "                    with gr.Tab(\"üíª Coding\"):\n",
    "                        difficulty = gr.Radio(choices=[\"Beginner\", \"Intermediate\", \"Advanced\"], \n",
    "                                            value=\"Beginner\", label=\"Difficulty\")\n",
    "                        gen_puzzle_btn = gr.Button(\"üéØ Generate Puzzle\")\n",
    "                        \n",
    "                        puzzle_desc = gr.Markdown()\n",
    "                        cpu_code = gr.Code(label=\"CPU Code (pandas)\", interactive=False)\n",
    "                        user_code = gr.Code(label=\"Your GPU Code (cuDF)\", language=\"python\")\n",
    "                        \n",
    "                        check_btn = gr.Button(\"‚úÖ Check Solution\")\n",
    "                        benchmark_btn = gr.Button(\"‚ö° Benchmark\")\n",
    "                        \n",
    "                        puzzle_feedback = gr.Markdown()\n",
    "                        puzzle_state = gr.State({})\n",
    "                        \n",
    "                        def generate_puzzle(diff):\n",
    "                            puzzle = generate_coding_puzzle(diff)\n",
    "                            desc = f\"**üéØ {diff} Challenge:** {puzzle['description']}\"\n",
    "                            return desc, puzzle['cpu_code'], \"\", puzzle\n",
    "                        \n",
    "                        def check_solution(user_code_input, puzzle):\n",
    "                            if not puzzle or not user_code_input:\n",
    "                                return \"Generate a puzzle and enter your solution!\"\n",
    "                            \n",
    "                            similarity = difflib.SequenceMatcher(None, \n",
    "                                                               user_code_input.strip(), \n",
    "                                                               puzzle['gpu_code'].strip()).ratio()\n",
    "                            \n",
    "                            if similarity > 0.8:\n",
    "                                user_state.points += 10\n",
    "                                return f\"üéâ Excellent! Perfect solution! +10 XP | Total: {user_state.points} XP\"\n",
    "                            elif similarity > 0.6:\n",
    "                                user_state.points += 5\n",
    "                                return f\"‚úÖ Good effort! Close solution. +5 XP | Total: {user_state.points} XP\\n\\nüí° **Hint:** Check your imports and variable names\"\n",
    "                            elif similarity > 0.4:\n",
    "                                user_state.points += 2\n",
    "                                return f\"üü° Partial credit for trying! +2 XP | Total: {user_state.points} XP\\n\\nüí° **Hints:**\\n‚Ä¢ Replace 'pandas' or 'pd' with 'cudf'\\n‚Ä¢ Check function names and syntax\\n‚Ä¢ Make sure to import cudf\"\n",
    "                            else:\n",
    "                                return f\"‚ùå Not quite right. No points awarded.\\n\\nüí° **Hints:**\\n‚Ä¢ Replace 'import pandas as pd' with 'import cudf'\\n‚Ä¢ Replace 'pd.DataFrame' with 'cudf.DataFrame'\\n‚Ä¢ Replace 'pd.' with 'cudf.' for other operations\\n\\n**Expected pattern:** {puzzle['gpu_code']}\"\n",
    "                        \n",
    "                        def run_benchmark(user_code_input, puzzle):\n",
    "                            if not puzzle:\n",
    "                                return \"Generate a puzzle first!\"\n",
    "                            \n",
    "                            similarity = difflib.SequenceMatcher(None, \n",
    "                                                               user_code_input.strip(), \n",
    "                                                               puzzle['gpu_code'].strip()).ratio()\n",
    "                            \n",
    "                            if similarity > 0.6:\n",
    "                                cpu_time = puzzle['cpu_time']\n",
    "                                gpu_time = puzzle['gpu_time']\n",
    "                                speedup = round(cpu_time / gpu_time, 1)\n",
    "                                return f\"‚ö° **Benchmark Results:**\\nüìä CPU: {cpu_time}s\\nüöÄ GPU: {gpu_time}s\\n‚ö° **Speedup: {speedup}x**\"\n",
    "                            else:\n",
    "                                return \"‚ùå Fix your solution first!\"\n",
    "                        \n",
    "                        gen_puzzle_btn.click(generate_puzzle, inputs=difficulty, \n",
    "                                           outputs=[puzzle_desc, cpu_code, user_code, puzzle_state])\n",
    "                        check_btn.click(check_solution, inputs=[user_code, puzzle_state], outputs=puzzle_feedback)\n",
    "                        benchmark_btn.click(run_benchmark, inputs=[user_code, puzzle_state], outputs=puzzle_feedback)\n",
    "                \n",
    "                # Update progress on point changes\n",
    "                def refresh_progress():\n",
    "                    return update_progress()\n",
    "                \n",
    "                # Connect progress updates\n",
    "                gen_quiz_btn.click(refresh_progress, outputs=progress)\n",
    "                check_btn.click(refresh_progress, outputs=progress)\n",
    "        \n",
    "        # Footer\n",
    "        gr.Markdown(\"\"\"\n",
    "        ---\n",
    "        <div style='text-align:center; color:#666; padding:15px;'>\n",
    "        üåü **AI Tutor Complete Learning System** ‚Ä¢ Structured Learning Paths ‚Ä¢ Socratic Guidance ‚Ä¢ Gamified Practice<br>\n",
    "        üìö <i>From beginner tutorials to advanced GPU computing - your complete data science learning companion!</i><br>\n",
    "        üí° **Learning Mode:** Topic-based structured courses | **Tutor Mode:** Socratic Q&A | **Game Mode:** XP-based practice\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    \n",
    "    return app\n",
    "\n",
    "# === LAUNCH ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ Creating AI Tutor interface...\")\n",
    "    app = create_ai_tutor()\n",
    "    \n",
    "    print(\"üéâ AI Tutor ready!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ Complete Learning System Available:\")\n",
    "    print(\"  üìö Learning Mode - Structured topic-based learning paths\")\n",
    "    print(\"  üéì Tutor Mode - Socratic Q&A with guided discovery\")\n",
    "    print(\"  üéÆ Game Mode - XP-based practice with flashcards, quizzes & coding\")\n",
    "    print(\"  üåü Dynamic content generation with contextual quotes\")\n",
    "    print(\"  üìñ Enhanced documentation (12+ sources)\")\n",
    "    print(\"  üéØ From beginner Python to advanced GPU computing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Launch the app\n",
    "    app.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9083f77-56de-485b-a482-7ec537c7e907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
