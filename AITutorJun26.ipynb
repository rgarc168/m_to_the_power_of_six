{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644ad06-7d61-4873-9633-c25600b3a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "# Includes: FAISS RAG system, fallback to Falcon-7B-Instruct, and full game mode\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import random\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "# --- CONFIG ---\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL = \"MiniMaxAI/SynLogic-7B\"\n",
    "#https://huggingface.co/tecosys/Nutaan-RL1\n",
    "#https://huggingface.co/knowledgator/Qwen-encoder-0.5B\n",
    "#https://huggingface.co/knowledgator/Llama-encoder-1.0B\n",
    "CHUNK_FILE = \"chunks.pkl\"\n",
    "INDEX_FILE = \"faiss.index\"\n",
    "URLS_FILE = \"custom_urls.txt\"\n",
    "\n",
    "ADDITIONAL_URLS = [\n",
    "    \"https://rapids.ai/\", \"https://rapids.ai/cudf-pandas/\", \"https://rapids.ai/cuml-accel/\",\n",
    "    \"https://cupy.dev/\", \"https://cuml.ai/\", \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "    \"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\",\n",
    "    \"https://scikit-learn.org/stable/\", \"https://pandas.pydata.org/\",\n",
    "    \"https://www.geeksforgeeks.org/data-science/data-science-for-beginners/\",\n",
    "    \"https://dlsyscourse.org/slides/12-gpu-acceleration.pdf\"\n",
    "]\n",
    "\n",
    "# --- Ensure URL Cache ---\n",
    "def ensure_custom_urls():\n",
    "    existing = set()\n",
    "    if os.path.exists(URLS_FILE):\n",
    "        with open(URLS_FILE, \"r\") as f:\n",
    "            existing = set(x.strip() for x in f)\n",
    "    with open(URLS_FILE, \"a\") as f:\n",
    "        for url in ADDITIONAL_URLS:\n",
    "            if url not in existing:\n",
    "                f.write(url + \"\\n\")\n",
    "ensure_custom_urls()\n",
    "\n",
    "# --- Load Models ---\n",
    "embedder = SentenceTransformer(EMBED_MODEL, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_MODEL,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=800,\n",
    "    do_sample=False,\n",
    "    return_full_text=False  # prevents prompt from being echoed back\n",
    ")\n",
    "\n",
    "# --- RAG Utilities ---\n",
    "def read_urls_from_txt(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (RAG Tutor Bot)'}\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "        # Filter out non-text content types\n",
    "        content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"text/\") and \"html\" not in content_type:\n",
    "            print(f\"⚠️ Skipping non-text URL: {url} (type={content_type})\")\n",
    "            return \"\"\n",
    "\n",
    "        # Parse HTML content safely\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "        return soup.get_text(separator=\"\\n\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "def load_all_chunks(urls):\n",
    "    all_chunks = []\n",
    "    for url in urls:\n",
    "        txt = fetch_text_from_url(url)\n",
    "        if txt:\n",
    "            all_chunks.extend(chunk_text(txt))\n",
    "    return all_chunks\n",
    "\n",
    "def embed_and_index(chunks):\n",
    "    vecs = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vecs))\n",
    "    return index\n",
    "\n",
    "def query_rag(query, index, chunks, k=3):\n",
    "    if index is None or not chunks:\n",
    "        return \"\", \"\"\n",
    "    q_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(np.array(q_vec), k)\n",
    "    return \"\\n\".join([chunks[i] for i in I[0]]), \"Custom Knowledge Base\"\n",
    "\n",
    "def load_cache():\n",
    "    if os.path.exists(CHUNK_FILE) and os.path.exists(INDEX_FILE):\n",
    "        with open(CHUNK_FILE, \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "        index = faiss.read_index(INDEX_FILE)\n",
    "        return chunks, index\n",
    "    return None, None\n",
    "\n",
    "def save_cache(chunks, index):\n",
    "    with open(CHUNK_FILE, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "\n",
    "def build_or_load_rag():\n",
    "    chunks, index = load_cache()\n",
    "    if not chunks or index is None:\n",
    "        urls = read_urls_from_txt(URLS_FILE)\n",
    "        chunks = load_all_chunks(urls)\n",
    "        index = embed_and_index(chunks)\n",
    "        save_cache(chunks, index)\n",
    "    return chunks, index\n",
    "\n",
    "chunks, index = build_or_load_rag()\n",
    "\n",
    "# --- Smart Tutor Answer: RAG + Fallback ---\n",
    "DOC_LINKS = {\n",
    "    # Core Libraries\n",
    "    \"pandas\": \"https://pandas.pydata.org/docs/\",\n",
    "    \"scikit-learn\": \"https://scikit-learn.org/stable/\",\n",
    "    \"rapids\": \"https://rapids.ai/\",\n",
    "    \"cudf\": \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "    \"cupy\": \"https://docs.cupy.dev/en/stable/\",\n",
    "    \"pytorch\": \"https://pytorch.org/docs/stable/\",\n",
    "    \"tensorflow\": \"https://www.tensorflow.org/\",\n",
    "    \"cuml\": \"https://docs.rapids.ai/api/cuml/stable/\",\n",
    "\n",
    "    # Data Science\n",
    "    \"dsml_pdf\": \"https://people.smp.uq.edu.au/DirkKroese/DSML/DSML.pdf\",\n",
    "    \"awesome_datascience\": \"https://github.com/academic/awesome-datascience?tab=readme-ov-file\",\n",
    "    \"gfg_ds_beginners\": \"https://www.geeksforgeeks.org/data-science/data-science-for-beginners/\",\n",
    "\n",
    "    # GPU Acceleration\n",
    "    \"cudf_pandas\": \"https://rapids.ai/cudf-pandas/\",\n",
    "    \"polars_gpu\": \"https://rapids.ai/polars-gpu-engine/\",\n",
    "    \"cuml_accel\": \"https://rapids.ai/cuml-accel/\",\n",
    "    \"nx_cugraph\": \"https://rapids.ai/nx-cugraph/\",\n",
    "    \"hagedorn2022\": \"https://proceedings.mlr.press/v185/hagedorn22a/hagedorn22a.pdf\",\n",
    "    \"nvidia_tesla_whitepaper\": \"https://www.nvidia.com/docs/io/116711/sc11-nv-tesla.pdf\",\n",
    "    \"cuda_blog\": \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "    \"libcxxgpu_pdf\": \"https://www.seas.upenn.edu/~delozier/docs/libcxxgpu.pdf\",\n",
    "    \"gpu_accel_slides\": \"https://dlsyscourse.org/slides/12-gpu-acceleration.pdf\",\n",
    "    \"gpucad_paper\": \"https://yibolin.com/publications/papers/GPUCAD_ICCAD2020_Lin.pdf\",\n",
    "    \"gpucad_slides\": \"https://yibolin.com/publications/papers/GPUCAD_ICCAD2020_Lin.slides.pdf\",\n",
    "    \"spie_gpu_sample\": \"https://www.spiedigitallibrary.org/samples/SL34.pdf\",\n",
    "    \"gpu_to_web\": \"https://www.khronos.org/assets/uploads/developers/library/2012-the-graphical-web/GPU-to-the-web_Sep2012.pdf\",\n",
    "\n",
    "    # RAG Retrieval\n",
    "    \"langgraph_agentic_rag\": \"https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\",\n",
    "\n",
    "    # Basics\n",
    "    \"gpu_compare\": \"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/\",\n",
    "    \"gpu_hierarchy\": \"https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html\"\n",
    "}\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# --- Smart Tutor Answer: RAG + Fallback ---\n",
    "DOC_LINKS = {\n",
    "    \"pandas\": \"https://pandas.pydata.org/docs/\",\n",
    "    \"scikit-learn\": \"https://scikit-learn.org/stable/\",\n",
    "    \"rapids\": \"https://rapids.ai/\",\n",
    "    \"cudf\": \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "    \"cupy\": \"https://docs.cupy.dev/en/stable/\",\n",
    "    \"pytorch\": \"https://pytorch.org/docs/stable/\",\n",
    "    \"tensorflow\": \"https://www.tensorflow.org/\",\n",
    "    \"cuml\": \"https://docs.rapids.ai/api/cuml/stable/\",\n",
    "}\n",
    "\n",
    "related_examples = {\n",
    "    \"gpu\": [\"What is CUDA?\", \"cuDF vs pandas?\", \"cuML for ML tasks?\"],\n",
    "    \"pandas\": [\"How to groupby in pandas?\", \"pandas vs cuDF?\", \"Time series in pandas\"],\n",
    "    \"scikit-learn\": [\"What is train_test_split?\", \"GridSearchCV usage\", \"PCA in sklearn\"]\n",
    "}\n",
    "\n",
    "def suggest_related(query):\n",
    "    suggestions = []\n",
    "    for keyword, rel_list in related_examples.items():\n",
    "        if re.search(rf\"\\\\b{keyword}\\\\b\", query, re.I):\n",
    "            suggestions = rel_list\n",
    "            break\n",
    "    if suggestions:\n",
    "        bullets = \"\".join([f\"- {q}<br>\" for q in suggestions])\n",
    "        return f\"<br><br><b>🔎 You might also ask:</b><br>{bullets}\"\n",
    "    return \"\"\n",
    "\n",
    "def generate_example_plot():\n",
    "    x = [100, 500, 1000, 5000, 10000]\n",
    "    cpu_times = [0.5, 1.8, 3.2, 15.0, 30.0]\n",
    "    gpu_times = [0.2, 0.5, 0.8, 2.5, 5.0]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=cpu_times, mode='lines+markers', name='CPU'))\n",
    "    fig.add_trace(go.Scatter(x=x, y=gpu_times, mode='lines+markers', name='GPU'))\n",
    "    fig.update_layout(title='CPU vs GPU Execution Time',\n",
    "                      xaxis_title='Input Size',\n",
    "                      yaxis_title='Time (s)',\n",
    "                      legend_title='Processor')\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    fig.write_image(buffer, format='png')\n",
    "    img_base64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "    return f'<img src=\"data:image/png;base64,{img_base64}\" style=\"width:100%;max-width:600px;\">'\n",
    "\n",
    "#TO-DO\n",
    "#def polish_response(text):\n",
    "    #prompt = f\"Polish this answer to be clear, helpful, and concise like a good AI tutor. Avoid repetition:\\n\\n{text}\\n\\nPolished Answer:\"\n",
    "def polish_response(text):\n",
    "    prompt = f\"\"\"\n",
    "You're an expert AI tutor for Data Science and GPU Acceleration. Take the content below and improve it into a well-structured explanation that is:\n",
    "\n",
    "- Friendly and clear\n",
    "- Easy for students to understand\n",
    "- Broken into logical sections\n",
    "- Uses analogies/examples when helpful\n",
    "- Suggests any charts or visualizations if relevant\n",
    "- Includes a short summary or takeaway at the end\n",
    "\n",
    "Original content:\n",
    "-------------------------------\n",
    "{text}\n",
    "-------------------------------\n",
    "\n",
    "Now improve and rewrite it in tutor style:\n",
    "\"\"\"\n",
    "    try:\n",
    "        raw = llm_pipeline(prompt)[0]['generated_text']\n",
    "        split = raw.split(\"Now improve and rewrite it in tutor style:\")\n",
    "        return split[-1].strip() if len(split) > 1 else raw.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ polish_response failed: {e}\")\n",
    "        return \"Sorry, I couldn’t process this content right now.\"\n",
    "    \n",
    "def smart_tutor_answer(query):\n",
    "    context, source = query_rag(query, index, chunks)\n",
    "\n",
    "    def is_garbage(text):\n",
    "        non_ascii = sum(1 for c in text if ord(c) > 126 or ord(c) < 9)\n",
    "        return len(text) < 80 or non_ascii / len(text) > 0.2\n",
    "\n",
    "    if not context or is_garbage(context):\n",
    "        print(\"⚠️ RAG returned bad or short text. Falling back to LLM.\")\n",
    "        prompt = f\"You're a concise, helpful AI tutor for data science and GPU acceleration. Answer this clearly:\\n\\nQ: {query}\\nA:\"\n",
    "        result = llm_pipeline(prompt)[0]['generated_text']\n",
    "        answer = result.strip().split(\"A:\")[-1].strip()\n",
    "\n",
    "        doc_link = \"\"\n",
    "        for k, v in DOC_LINKS.items():\n",
    "            if re.search(rf\"\\\\b{k}\\\\b\", query, re.I):\n",
    "                doc_link = f\"\\n\\n🔗 [Official {k.title()} Docs]({v})\"\n",
    "                source = v\n",
    "                break\n",
    "        graph = generate_example_plot() if \"gpu\" in query.lower() or \"benchmark\" in query.lower() else \"\"\n",
    "        return answer + doc_link + suggest_related(query) + \"<br><br>\" + graph, source if source else \"LLM (fallback)\"\n",
    "\n",
    "    else:\n",
    "        polished = polish_response(context.strip())\n",
    "        graph = generate_example_plot() if \"gpu\" in query.lower() or \"benchmark\" in query.lower() else \"\"\n",
    "        extras = suggest_related(query)\n",
    "        return f\"{polished}<br><br>{extras}<br><br>{graph}\", source if source else \"Custom Knowledge Base\"\n",
    "# === Place UI Code BELOW This Line ===\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"M^6 AI Tutor\") as app:\n",
    "    gr.Markdown(\"\"\"\n",
    "    <div style='text-align:center; background:linear-gradient(90deg,#fff1c1,#c1e7ff,#e1ffc1); border-radius:15px; padding:10px 0; margin-bottom:8px; color:#000;'>\n",
    "      <h1 style='color:#000;'>🧠 M^6 AI Tutor</h1>\n",
    "      <h3 style='color:#000;'>Switch between <span style='color:#0a88a4'>Tutor Mode</span> and <span style='color:#0a88a4'>Game Mode</span> for interactive learning!</h3>\n",
    "      <p style='color:#111; font-weight:700; font-size:1.05em;'>\n",
    "        <b>Learn, Play, and Benchmark real data science and GPU acceleration!</b>\n",
    "      </p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Tutor Mode\"):\n",
    "            tutor_query = gr.Textbox(label=\"Ask about Data Science or GPU topics\", placeholder=\"e.g., What is RAPIDS?\")\n",
    "            tutor_answer_box = gr.Markdown(label=\"AI Tutor's Answer\", value=\"\")\n",
    "            tutor_source_box = gr.Markdown(label=\"Source of Answer\", value=\"\")\n",
    "            tutor_btn = gr.Button(\"🚀 Get Tutor Help\")\n",
    "            def tutor_handler(q):\n",
    "                answer, source = smart_tutor_answer(q)\n",
    "                return answer, f\"**Source:** {source}\"\n",
    "            tutor_btn.click(fn=tutor_handler, inputs=tutor_query, outputs=[tutor_answer_box, tutor_source_box])\n",
    "\n",
    "        with gr.Tab(\"Game Mode\"):\n",
    "            from types import SimpleNamespace\n",
    "            user_state = SimpleNamespace(points=0, level=1)\n",
    "\n",
    "            def update_level():\n",
    "                user_state.level = min(10, 1 + user_state.points // 10)\n",
    "                bar = f\"\"\"<b>⭐ {user_state.points} points — Level {user_state.level}</b><br>\n",
    "                <div style='background:#eee;border-radius:8px;width:100%;height:18px;'>\n",
    "                <div style='background:linear-gradient(90deg,#fceabb,#f8b500);height:18px;width:{(user_state.points % 10)*10}%;border-radius:8px;'></div>\n",
    "                </div>\"\"\"\n",
    "                return bar\n",
    "\n",
    "            progress = gr.HTML(update_level())\n",
    "\n",
    "            FLASHCARDS = [\n",
    "                {\"front\": \"What is cuDF?\", \"back\": \"A RAPIDS GPU DataFrame library, similar to pandas but on GPU.\"},\n",
    "                {\"front\": \"What is a CUDA core?\", \"back\": \"A processing unit on an NVIDIA GPU for parallel computing.\"},\n",
    "                {\"front\": \"Why use RAPIDS?\", \"back\": \"RAPIDS enables GPU-accelerated DataFrame operations similar to pandas.\"},\n",
    "                {\"front\": \"How do you create a DataFrame in pandas?\", \"back\": \"import pandas as pd; df = pd.DataFrame(...)\"}\n",
    "            ]\n",
    "            def get_flashcard(idx, show):\n",
    "                card = FLASHCARDS[idx % len(FLASHCARDS)]\n",
    "                front = f\"🃏 Q: {card['front']}\"\n",
    "                back = f\"A: {card['back']}\" if show else \"Click 'Show Answer'\"\n",
    "                return front, back, f\"Card {idx+1}/{len(FLASHCARDS)}\", idx % len(FLASHCARDS), show\n",
    "\n",
    "            with gr.Tabs():\n",
    "                with gr.Tab(\"🃏 Flashcards\"):\n",
    "                    flash_idx = gr.State(0)\n",
    "                    show_answer = gr.State(False)\n",
    "                \n",
    "                    FLASHCARDS = [\n",
    "                        {\"front\": \"What is cuDF?\", \"back\": \"A RAPIDS GPU DataFrame library, similar to pandas but on GPU.\"},\n",
    "                        {\"front\": \"What is a CUDA core?\", \"back\": \"A processing unit on an NVIDIA GPU for parallel computing.\"},\n",
    "                        {\"front\": \"Why use RAPIDS?\", \"back\": \"RAPIDS enables GPU-accelerated DataFrame operations similar to pandas.\"},\n",
    "                        {\"front\": \"How do you create a DataFrame in pandas?\", \"back\": \"import pandas as pd; df = pd.DataFrame(...)\"}\n",
    "                    ]\n",
    "                \n",
    "                    def get_flashcard(idx, show):\n",
    "                        card = FLASHCARDS[idx % len(FLASHCARDS)]\n",
    "                        front = f\"🃏 Q: {card['front']}\"\n",
    "                        back = f\"A: {card['back']}\" if show else \"\"\n",
    "                        count = f\"Card {idx+1}/{len(FLASHCARDS)}\"\n",
    "                        return front, back, count, idx % len(FLASHCARDS), show\n",
    "                \n",
    "                    # UI components\n",
    "                    card_picker = gr.Dropdown(choices=[f\"Card {i+1}\" for i in range(len(FLASHCARDS))], label=\"Choose a Flashcard\", value=\"Card 1\")\n",
    "                    card_front = gr.Markdown()\n",
    "                    card_back = gr.Markdown()\n",
    "                    card_count = gr.Markdown()\n",
    "                \n",
    "                    flip_btn = gr.Button(\"🔄 Flip Card\")\n",
    "                    reset_btn = gr.Button(\"🔁 Reset\")\n",
    "                \n",
    "                    # Logic\n",
    "                    def pick_card(label):\n",
    "                        idx = int(label.split()[-1]) - 1\n",
    "                        return get_flashcard(idx, False)\n",
    "                \n",
    "                    def flip_card(idx, show):\n",
    "                        return get_flashcard(idx, not show)\n",
    "                \n",
    "                    def reset_flashcards():\n",
    "                        return get_flashcard(0, False)\n",
    "                \n",
    "                    card_picker.change(pick_card, inputs=card_picker, outputs=[card_front, card_back, card_count, flash_idx, show_answer])\n",
    "                    flip_btn.click(flip_card, inputs=[flash_idx, show_answer], outputs=[card_front, card_back, card_count, flash_idx, show_answer])\n",
    "                    reset_btn.click(reset_flashcards, outputs=[card_front, card_back, card_count, flash_idx, show_answer])\n",
    "                                    \n",
    "                with gr.Tab(\"❓ Quiz\"):\n",
    "                    with gr.Tab(\"❓ Quiz\"):\n",
    "                        quiz_pool = [\n",
    "                            (\"Which library is used for GPU DataFrame processing?\", [\"pandas\", \"numpy\", \"RAPIDS\", \"sklearn\"], 2),\n",
    "                            (\"Who develops CUDA?\", [\"AMD\", \"NVIDIA\", \"Intel\", \"Google\"], 1)\n",
    "                        ]\n",
    "                        quiz_state = gr.State(quiz_pool)\n",
    "                    \n",
    "                        # Markdown components to display the questions\n",
    "                        quiz_question_1 = gr.Markdown(value=f\"**Q1:** {quiz_pool[0][0]}\")\n",
    "                        quiz_radio_1 = gr.Radio(choices=quiz_pool[0][1], label=\"\")\n",
    "                    \n",
    "                        quiz_question_2 = gr.Markdown(value=f\"**Q2:** {quiz_pool[1][0]}\")\n",
    "                        quiz_radio_2 = gr.Radio(choices=quiz_pool[1][1], label=\"\")\n",
    "                    \n",
    "                        quiz_btn = gr.Button(\"Submit Quiz\")\n",
    "                        quiz_out = gr.Markdown()\n",
    "                    \n",
    "                        def evaluate_quiz(ans1, ans2, pool):\n",
    "                            correct = 0\n",
    "                            if ans1 == pool[0][1][pool[0][2]]:\n",
    "                                correct += 1\n",
    "                            if ans2 == pool[1][1][pool[1][2]]:\n",
    "                                correct += 1\n",
    "                            user_state.points += correct * 3\n",
    "                            result = f\"✅ {correct}/2 Correct. +{correct*3} pts\"\n",
    "                            return result, update_level()\n",
    "                    \n",
    "                        quiz_btn.click(fn=evaluate_quiz, inputs=[quiz_radio_1, quiz_radio_2, quiz_state], outputs=[quiz_out, progress])\n",
    "    \n",
    "\n",
    "                with gr.Tab(\"💻 Coding Puzzle\"):\n",
    "                    gr.Markdown(\"<b>Benchmark CPU vs GPU</b><br>Check your cuDF/cuML code accuracy and visualize speedup.\")\n",
    "\n",
    "                    puzzle = {\n",
    "                        \"desc\": \"Convert Pandas CPU code to cuDF GPU\",\n",
    "                        \"cpu\": \"import pandas as pd\\n\"\n",
    "                               \"df = pd.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\",\n",
    "                        \"gpu\": \"import cudf\\n\"\n",
    "                               \"df = cudf.DataFrame({'a': [1,2,3]})\\n\"\n",
    "                               \"print(df.sum())\"\n",
    "                    }\n",
    "\n",
    "                    cpu_code = gr.Code(label=\"CPU Code\", value=puzzle['cpu'], interactive=False)\n",
    "                    user_code = gr.Code(label=\"Your GPU Code\", language=\"python\")\n",
    "                \n",
    "                    check_btn = gr.Button(\"Check Solution 🚦\")\n",
    "                    bench_btn = gr.Button(\"Run Benchmark 🚀\")\n",
    "                    feedback = gr.Markdown()\n",
    "                    bench_out = gr.Markdown()\n",
    "                \n",
    "                    def check_gpu_code(code):\n",
    "                        import difflib\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            user_state.points += 5\n",
    "                            return \"🎉 Looks good! +5 points\", update_level()\n",
    "                        else:\n",
    "                            return \"❌ Not quite. Try matching the structure of cuDF.\", update_level()\n",
    "                \n",
    "                    def run_benchmark(code):\n",
    "                        match = difflib.SequenceMatcher(None, code.strip(), puzzle['gpu'].strip()).ratio()\n",
    "                        if match > 0.7:\n",
    "                            cpu_time = 1.5\n",
    "                            gpu_time = 0.2\n",
    "                            speedup = round(cpu_time / gpu_time, 1)\n",
    "                            return f\"⏱️ CPU time: {cpu_time}s<br>⚡ GPU time: {gpu_time}s<br>🚀 Speedup: {speedup}x\"\n",
    "                        else:\n",
    "                            return \"❌ Code not GPU-convertible. Fix and try again.\"\n",
    "                \n",
    "                    check_btn.click(check_gpu_code, [user_code], [feedback, progress])\n",
    "                    bench_btn.click(run_benchmark, [user_code], [bench_out])\n",
    "\n",
    "app.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
