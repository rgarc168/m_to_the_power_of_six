{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50a2e04-a02a-4021-a3df-7e53b93f6dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ GPU DETECTED: NVIDIA A100-SXM4-80GB\n",
      "üìä GPU Memory: 79.3 GB\n",
      "üî¢ GPU Count: 1\n",
      "‚úÖ GPU is available and functional\n",
      "üîß Loading SentenceTransformer on CUDA...\n",
      "‚úÖ SentenceTransformer loaded successfully on CUDA\n",
      "ü§ñ Loading NousResearch/Hermes-2-Pro-Mistral-7B\n",
      "üöÄ Loading model on GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97476c65a3c34e71a01c0bd5fbdba425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully on GPU\n",
      "‚ö†Ô∏è cuDF not installed - will use pandas fallback in benchmarks\n",
      "\n",
      "üîß **DEVICE CONFIGURATION SUMMARY:**\n",
      "üì± Main Device: CUDA\n",
      "üöÄ GPU Available: ‚úÖ Yes\n",
      "üñ•Ô∏è Device Name: NVIDIA A100-SXM4-80GB\n",
      "üíæ GPU Memory: 79.3 GB\n",
      "üìä cuDF Status: ‚ö†Ô∏è Using pandas fallback\n",
      "üéØ Benchmarking Mode: Simulated with pandas\n",
      "--------------------------------------------------\n",
      "‚úÖ LLM loaded and ready!\n",
      "üöÄ Initializing RAG system...\n",
      "üîÑ Initializing RAG system...\n",
      "‚úÖ Loaded cached RAG with 1005 chunks\n",
      "‚úÖ Using cached RAG with 1005 chunks\n",
      "\n",
      "üöÄ Creating AI Tutor interface...\n",
      "üéâ AI Tutor ready!\n",
      "============================================================\n",
      "‚úÖ Complete Learning System Available:\n",
      "  üìö Learning Mode - Structured topic-based learning paths\n",
      "  üéì Tutor Mode - Socratic Q&A with guided discovery\n",
      "  üéÆ Game Mode - XP-based practice with flashcards, quizzes & coding\n",
      "  üåü Dynamic content generation with contextual quotes\n",
      "  üìñ Enhanced documentation (12+ sources)\n",
      "  üéØ From beginner Python to advanced GPU computing\n",
      "============================================================\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://48be6be610a01d384b.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://48be6be610a01d384b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üöÄ AI Tutor - Clean Working DataSci GPU Tutor (RAG + LLM + Game Mode)\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import requests\n",
    "import random\n",
    "import difflib\n",
    "import re\n",
    "from types import SimpleNamespace\n",
    "import ast\n",
    "import time\n",
    "import cProfile\n",
    "import io\n",
    "import pstats\n",
    "import contextlib\n",
    "import sys\n",
    "import traceback\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "# Includes: FAISS RAG system, fallback to Falcon-7B-Instruct, and full game mode\n",
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "# EMBED_MODEL = \"/scratch/ntiwar12/huggingfacesentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBED_MODEL = \"/scratch/ntiwar12/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/\"\n",
    "#\"BAAI/bge-m3\"\n",
    "#\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL = \"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "#\"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "#\"microsoft/phi-4\"\n",
    "# LLM_MODEL = \"microsoft/DialoGPT-small\"  # Much smaller model\n",
    "#EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# LLM_MODEL = \"MiniMaxAI/SynLogic-7B\"\n",
    "# LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    " \n",
    "#\"meta-llama/Meta-Llama-3-8B\"\n",
    "#\"allenai/digital-socrates-13b\"\n",
    "#\"/scratch/ntiwar12/huggingface/hub/models--WizardLM--WizardCoder-Python-34B-V1.0/snapshots/897fc6d9e12136c68c441b2350d015902c144b20/\"\n",
    "#\"allenai/digital-socrates-13b\"\n",
    "\n",
    "\n",
    "#\"NousResearch/Hermes-2-Pro-Mistral-7B\"\n",
    "#https://huggingface.co/tecosys/Nutaan-RL1\n",
    "#https://huggingface.co/knowledgator/Qwen-encoder-0.5B\n",
    "#https://huggingface.co/knowledgator/Llama-encoder-1.0B\n",
    "CHUNK_FILE = \"chunks.pkl\"\n",
    "INDEX_FILE = \"faiss.index\" \n",
    "URLS_FILE = \"custom_urls.txt\"\n",
    "\n",
    "# --- Device Detection and Setup ---\n",
    "def detect_and_setup_device():\n",
    "    \"\"\"Detect available device and configure models accordingly\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else \"Unknown GPU\"\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3) if gpu_count > 0 else 0\n",
    "        \n",
    "        print(f\"üöÄ GPU DETECTED: {gpu_name}\")\n",
    "        print(f\"üìä GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        print(f\"üî¢ GPU Count: {gpu_count}\")\n",
    "        \n",
    "        try:\n",
    "            # Test GPU availability with a small tensor operation\n",
    "            test_tensor = torch.randn(10, 10).cuda()\n",
    "            _ = test_tensor @ test_tensor\n",
    "            print(\"‚úÖ GPU is available and functional\")\n",
    "            return \"cuda\", True, gpu_name, gpu_memory\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è GPU detected but not functional: {e}\")\n",
    "            print(\"üîÑ Falling back to CPU...\")\n",
    "            return \"cpu\", False, \"CPU (GPU fallback)\", 0\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è No GPU detected - using CPU\")\n",
    "        return \"cpu\", False, \"CPU\", 0\n",
    "\n",
    "# Detect device and get configuration\n",
    "device, has_gpu, device_name, gpu_memory = detect_and_setup_device()\n",
    "\n",
    "# Configure embedder with device detection\n",
    "try:\n",
    "    print(f\"üîß Loading SentenceTransformer on {device.upper()}...\")\n",
    "    embedder = SentenceTransformer(EMBED_MODEL, device=device)\n",
    "    print(f\"‚úÖ SentenceTransformer loaded successfully on {device.upper()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to load SentenceTransformer on {device}: {e}\")\n",
    "    if device == \"cuda\":\n",
    "        print(\"üîÑ Falling back to CPU for embedder...\")\n",
    "        embedder = SentenceTransformer(EMBED_MODEL, device='cpu')\n",
    "        device = \"cpu\"\n",
    "        has_gpu = False\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Configure LLM with device detection\n",
    "print(f\"ü§ñ Loading {LLM_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, trust_remote_code=True)\n",
    "\n",
    "try:\n",
    "    if has_gpu:\n",
    "        print(f\"üöÄ Loading model on GPU: {device_name}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_MODEL,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cuda\",\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded successfully on GPU\")\n",
    "    else:\n",
    "        print(\"üñ•Ô∏è Loading model on CPU (GPU not available)\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_MODEL,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded successfully on CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to load model on {device}: {e}\")\n",
    "    if has_gpu:\n",
    "        print(\"üîÑ Falling back to CPU for model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_MODEL,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "        has_gpu = False\n",
    "        device = \"cpu\"\n",
    "        print(\"‚úÖ Model loaded successfully on CPU (fallback)\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# --- cuDF Availability Detection ---\n",
    "def detect_cudf_availability():\n",
    "    \"\"\"Detect if cuDF is available and functional on GPU\"\"\"\n",
    "    global has_gpu\n",
    "    try:\n",
    "        import cudf\n",
    "        if has_gpu:\n",
    "            # Test cuDF GPU functionality with a simple operation\n",
    "            test_df = cudf.DataFrame({'test': [1, 2, 3]})\n",
    "            _ = test_df.sum()\n",
    "            print(\"‚úÖ cuDF is available and functional on GPU\")\n",
    "            return True, \"GPU\"\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è cuDF available but no GPU detected - will use pandas fallback\")\n",
    "            return False, \"CPU\"\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è cuDF not installed - will use pandas fallback in benchmarks\")\n",
    "        return False, \"CPU\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è cuDF detected but not functional on GPU: {e}\")\n",
    "        print(\"üîÑ Will use pandas fallback in benchmarks\")\n",
    "        return False, \"CPU\"\n",
    "\n",
    "# Check cuDF availability\n",
    "cudf_available, cudf_device = detect_cudf_availability()\n",
    "\n",
    "# Store device information for use throughout the application\n",
    "device_info = {\n",
    "    'main_device': device,\n",
    "    'has_gpu': has_gpu,\n",
    "    'device_name': device_name,\n",
    "    'gpu_memory': gpu_memory,\n",
    "    'cudf_available': cudf_available,\n",
    "    'cudf_device': cudf_device\n",
    "}\n",
    "\n",
    "print(f\"\\nüîß **DEVICE CONFIGURATION SUMMARY:**\")\n",
    "print(f\"üì± Main Device: {device.upper()}\")\n",
    "print(f\"üöÄ GPU Available: {'‚úÖ Yes' if has_gpu else '‚ùå No'}\")\n",
    "print(f\"üñ•Ô∏è Device Name: {device_name}\")\n",
    "if has_gpu:\n",
    "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "print(f\"üìä cuDF Status: {'‚úÖ Available on GPU' if cudf_available else '‚ö†Ô∏è Using pandas fallback'}\")\n",
    "print(f\"üéØ Benchmarking Mode: {'Real GPU acceleration' if cudf_available else 'Simulated with pandas'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ") \n",
    "\n",
    "\n",
    "def stream_llm_response(prompt, max_new_tokens=1000, temperature=0.7):\n",
    "    \"\"\"Generate streaming response from LLM\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Initialize generation parameters\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            \n",
    "            # Generate tokens one by one\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=temperature,\n",
    "                    pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "                \n",
    "                # Get the new token\n",
    "                new_token_id = outputs.sequences[0][-1].item()\n",
    "                \n",
    "                # Decode the new token\n",
    "                new_token = tokenizer.decode([new_token_id], skip_special_tokens=True)\n",
    "                \n",
    "                # Check for end of sequence\n",
    "                if new_token_id == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                \n",
    "                # Update inputs for next generation\n",
    "                input_ids = outputs.sequences\n",
    "                attention_mask = torch.cat([\n",
    "                    attention_mask, \n",
    "                    torch.ones((1, 1), device=model.device)\n",
    "                ], dim=1)\n",
    "                \n",
    "                yield new_token\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Streaming generation failed: {e}\")\n",
    "        yield \"Sorry, I encountered an error while generating the response.\"\n",
    "\n",
    "print(\"‚úÖ LLM loaded and ready!\")\n",
    "\n",
    "# === RAG UTILITIES ===\n",
    "\n",
    "def read_urls_from_txt(path):\n",
    "    \"\"\"Read URLs from a text file\"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è URLs file not found: {path}. Creating default URLs...\")\n",
    "        create_default_urls_file(path)\n",
    "        return read_urls_from_txt(path)\n",
    "\n",
    "\n",
    "def create_default_urls_file(path):\n",
    "    \"\"\"Create a default URLs file with common data science resources\"\"\"\n",
    "    default_urls = [\n",
    "        \"https://pandas.pydata.org/docs/\",\n",
    "        \"https://docs.rapids.ai/api/cudf/stable/\",\n",
    "        \"https://rapids.ai/\",\n",
    "        \"https://scikit-learn.org/stable/\",\n",
    "        \"https://pytorch.org/docs/stable/\",\n",
    "        \"https://www.tensorflow.org/\",\n",
    "        \"https://developer.nvidia.com/blog/tag/cuda/\",\n",
    "        \"https://people.smp.uq.edu.au/DirkKroese/DSML/DSML.pdf\"\n",
    "    ]\n",
    "    with open(path, \"w\") as f:\n",
    "        for url in default_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "    print(f\"‚úÖ Created default URLs file: {path}\")\n",
    "\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    \"\"\"Fetch and clean text content from a URL\"\"\"\n",
    "    try:\n",
    "        print(f\"üåê Fetching: {url}\")\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (RAG Tutor Bot)'}\n",
    "        resp = requests.get(url, headers=headers, timeout=15)\n",
    "        resp.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        # Filter out non-text content types\n",
    "        content_type = resp.headers.get(\"Content-Type\", \"\")\n",
    "        if not content_type.startswith(\"text/\") and \"html\" not in content_type:\n",
    "            print(f\"‚ö†Ô∏è Skipping non-text URL: {url} (type={content_type})\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Parse HTML content safely\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        text = soup.get_text(separator=\"\\n\").strip()\n",
    "        print(f\"‚úÖ Extracted {len(text)} characters from {url}\")\n",
    "        return text\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Network error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "\n",
    "\n",
    "def load_all_chunks(urls):\n",
    "    \"\"\"Load and chunk text from all URLs\"\"\"\n",
    "    all_chunks = []\n",
    "    for url in urls:\n",
    "        print(f\"üì• Processing: {url}\")\n",
    "        txt = fetch_text_from_url(url)\n",
    "        if txt:\n",
    "            chunks = chunk_text(txt)\n",
    "            all_chunks.extend(chunks)\n",
    "            print(f\"‚úÖ Added {len(chunks)} chunks from {url}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No content from {url}\")\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "def embed_and_index(chunks):\n",
    "    \"\"\"Create embeddings and FAISS index\"\"\"\n",
    "    if not chunks:\n",
    "        print(\"‚ö†Ô∏è No chunks to embed\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîÑ Creating embeddings for {len(chunks)} chunks...\")\n",
    "    vecs = embedder.encode(chunks, show_progress_bar=True, convert_to_numpy=True)\n",
    "    dim = vecs.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(vecs))\n",
    "    print(f\"‚úÖ Created FAISS index with {len(chunks)} chunks\")\n",
    "    return index\n",
    "\n",
    "\n",
    "def query_rag(query, index, chunks, k=3):\n",
    "    \"\"\"Query the RAG system\"\"\"\n",
    "    if index is None or not chunks:\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    q_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(np.array(q_vec), k)\n",
    "    retrieved_chunks = [chunks[i] for i in I[0] if i < len(chunks)]\n",
    "    return \"\\n\".join(retrieved_chunks), \"Custom Knowledge Base\"\n",
    "\n",
    "def load_cache():\n",
    "    \"\"\"Load cached chunks and index\"\"\"\n",
    "    if os.path.exists(CHUNK_FILE) and os.path.exists(INDEX_FILE):\n",
    "        try:\n",
    "            with open(CHUNK_FILE, \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "            index = faiss.read_index(INDEX_FILE)\n",
    "            print(f\"‚úÖ Loaded cached RAG with {len(chunks)} chunks\")\n",
    "            return chunks, index\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading cache: {e}\")\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "def save_cache(chunks, index):\n",
    "    \"\"\"Save chunks and index to cache\"\"\"\n",
    "    try:\n",
    "        with open(CHUNK_FILE, \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        faiss.write_index(index, INDEX_FILE)\n",
    "        print(f\"‚úÖ Cached RAG with {len(chunks)} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving cache: {e}\")\n",
    "\n",
    "def build_or_load_rag():\n",
    "    \"\"\"Build or load RAG system\"\"\"\n",
    "    print(\"üîÑ Initializing RAG system...\")\n",
    "    chunks, index = load_cache()\n",
    "    \n",
    "    if chunks and index is not None and len(chunks) > 0:\n",
    "        print(f\"‚úÖ Using cached RAG with {len(chunks)} chunks\")\n",
    "        return chunks, index\n",
    "    \n",
    "    print(\"üîÑ Building new RAG index...\")\n",
    "    \n",
    "    # Check if URLs file exists\n",
    "    if not os.path.exists(URLS_FILE):\n",
    "        print(f\"‚ö†Ô∏è URLs file not found: {URLS_FILE}\")\n",
    "        create_default_urls_file(URLS_FILE)\n",
    "    \n",
    "    urls = read_urls_from_txt(URLS_FILE)\n",
    "    print(f\"üìã Found {len(urls)} URLs to process\")\n",
    "    \n",
    "    chunks = load_all_chunks(urls)\n",
    "    print(f\"üì¶ Total chunks extracted: {len(chunks)}\")\n",
    "    \n",
    "    if not chunks or len(chunks) == 0:\n",
    "        print(\"‚ùå No chunks loaded, RAG will be disabled\")\n",
    "        return [], None\n",
    "    \n",
    "    # Filter out very short chunks\n",
    "    chunks = [chunk for chunk in chunks if len(chunk.strip()) > 50]\n",
    "    print(f\"üì¶ Filtered chunks: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        index = embed_and_index(chunks)\n",
    "        if index is not None:\n",
    "            save_cache(chunks, index)\n",
    "            print(f\"‚úÖ RAG system built successfully with {len(chunks)} chunks\")\n",
    "            return chunks, index\n",
    "        else:\n",
    "            print(\"‚ùå Failed to create embeddings\")\n",
    "            return [], None\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No valid chunks after filtering, RAG will be disabled\")\n",
    "        return [], None\n",
    "\n",
    "def is_garbage(text):\n",
    "    \"\"\"Check if text is low quality or corrupted\"\"\"\n",
    "    if not text or len(text) < 80:\n",
    "        return True\n",
    "    non_ascii = sum(1 for c in text if ord(c) > 126 or ord(c) < 9)\n",
    "    return non_ascii / len(text) > 0.2\n",
    "\n",
    "\n",
    "# Initialize RAG system\n",
    "print(\"üöÄ Initializing RAG system...\")\n",
    "rag_chunks, rag_index = build_or_load_rag()\n",
    "\n",
    "# === CORE FUNCTIONS ===\n",
    "\n",
    "def generate_contextual_quote(context=\"learning\", user_query=\"\"):\n",
    "    \"\"\"Generate contextual inspirational quotes\"\"\"\n",
    "    \n",
    "    # Try LLM generation first\n",
    "    if llm_pipeline:\n",
    "        try:\n",
    "            topic = extract_topic_from_query(user_query) if user_query else context\n",
    "            prompt = f\"Generate an inspiring quote about {topic} and learning:\"\n",
    "            result = llm_pipeline(prompt, max_new_tokens=50, temperature=0.8)\n",
    "            if result and len(result) > 0:\n",
    "                quote = result[0]['generated_text'].strip()\n",
    "                if len(quote) > 10:\n",
    "                    return f\"üß† '{quote}' - AI Generated\"\n",
    "        except Exception as e:\n",
    "            print(f\"Quote generation error: {e}\")\n",
    "    \n",
    "    # Enhanced fallback quotes\n",
    "    quotes = [\n",
    "        \"üí° 'Data is the new oil, but insights are the refined fuel.' - Anonymous\",\n",
    "        \"üöÄ 'The best way to get started is to quit talking and begin doing.' - Walt Disney\",\n",
    "        \"üß† 'Machine learning is the last invention that humanity will ever need to make.' - Nick Bostrom\",\n",
    "        \"‚ö° 'GPU acceleration: Think parallel, compute faster!' - AI Generated\",\n",
    "        \"üìä 'In data we trust, but insights we must discover!' - AI Generated\",\n",
    "        \"üî¨ 'Every algorithm learns from data, just like we learn from experience!' - AI Generated\",\n",
    "        \"üéØ 'The goal is to turn data into information, and information into insight.' - Carly Fiorina\",\n",
    "        \"üåü 'Artificial intelligence is the new electricity.' - Andrew Ng\",\n",
    "        \"‚öôÔ∏è 'The key to artificial intelligence has always been the representation.' - Jeff Hawkins\"\n",
    "    ]\n",
    "    return random.choice(quotes)\n",
    "\n",
    "\n",
    "def extract_topic_from_query(query):\n",
    "    \"\"\"Extract main topic from user query\"\"\"\n",
    "    if not query:\n",
    "        return \"learning\"\n",
    "    \n",
    "    query_lower = query.lower()\n",
    "    topic_keywords = {\n",
    "        'gpu computing': ['gpu', 'cuda', 'parallel', 'graphics'],\n",
    "        'data science': ['data', 'science', 'analytics', 'statistics'],\n",
    "        'machine learning': ['machine learning', 'ml', 'ai', 'artificial intelligence'],\n",
    "        'pandas': ['pandas', 'dataframe'],\n",
    "        'rapids': ['rapids', 'cudf', 'cuml'],\n",
    "        'programming': ['python', 'code', 'programming']\n",
    "    }\n",
    "    \n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        if any(keyword in query_lower for keyword in keywords):\n",
    "            return topic\n",
    "    return 'learning'\n",
    "\n",
    "\n",
    "def smart_tutor_answer_streaming(query):\n",
    "    \"\"\"Enhanced RAG-powered Socratic tutor responses with streaming\"\"\"\n",
    "    if not query or query.strip() == \"\":\n",
    "        yield \"ü§î Please ask me a question about data science, machine learning, or GPU computing!\"\n",
    "        return\n",
    "    \n",
    "    # Try RAG first\n",
    "    if rag_index is not None and rag_chunks:\n",
    "        context, source = query_rag(query, rag_index, rag_chunks)\n",
    "        \n",
    "        if context and not is_garbage(context):\n",
    "            # Stream the polished response\n",
    "            polished_prompt = f\"\"\"\n",
    "You're a semi-Socratic AI tutor for Data Science and GPU Acceleration. Take the content below and improve it into a well-structured explanation that is:\n",
    "\n",
    "- Friendly and clear\n",
    "- Easy for students to understand\n",
    "- Broken into small logical sections. \n",
    "- Ask questions before you answer. Make them think.\n",
    "- Uses analogies/examples when helpful\n",
    "- Suggests any charts or visualizations if relevant\n",
    "- Includes a short summary or takeaway at the end\n",
    "\n",
    "Original content:\n",
    "-------------------------------\n",
    "{context.strip()}\n",
    "-------------------------------\n",
    "\n",
    "Now improve and rewrite it in tutor style:\n",
    "\"\"\"\n",
    "            for token in stream_llm_response(polished_prompt, max_new_tokens=1500, temperature=0.7):\n",
    "                yield token\n",
    "            return\n",
    "    \n",
    "    # Fallback to LLM streaming for non-RAG responses\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Generic response with LLM streaming\n",
    "    if llm_pipeline:\n",
    "        try:\n",
    "            prompt = f\"\"\"You are a thoughtful, semi-socratic, and concise AI tutor specialized in data science and GPU acceleration.\n",
    "    \n",
    "    Your goal is to guide the learner with an engaging and structured explanation. \n",
    "    - Start by briefly asking a clarifying or reflective question (Socratic style)\n",
    "    - Then answer clearly, avoiding jargon. Keep answers short.\n",
    "    - Use examples or analogies if helpful\n",
    "    \n",
    "    Q: {query}\n",
    "    A:\"\"\"\n",
    "            for token in stream_llm_response(prompt, max_new_tokens=1000, temperature=0.7):\n",
    "                yield token\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå LLM fallback failed: {e}\")\n",
    "\n",
    "    yield f\"\"\"ü§î Let's explore this together:\n",
    "\n",
    "‚Ä¢ What do you already know about {query}?\n",
    "‚Ä¢ How might this relate to data science or computing?\n",
    "‚Ä¢ What would you want to accomplish by understanding this better?\n",
    "\n",
    "üí° **Hint:** Break down the concept into smaller parts and think about how each works.\"\"\"\n",
    "\n",
    "def generate_flashcards(topic, n=3):\n",
    "    \"\"\"Generate educational flashcards with RAG enhancement\"\"\"\n",
    "    \n",
    "    # Try RAG-based generation first\n",
    "    if rag_index is not None and rag_chunks:\n",
    "        try:\n",
    "            context, source = query_rag(topic, rag_index, rag_chunks)\n",
    "            if context and len(context.strip()) > 100:\n",
    "                prompt = f\"\"\"\n",
    "Create {n} educational flashcards about {topic} from this content. Format as JSON array:\n",
    "[{{\"front\": \"question\", \"back\": \"answer\"}}]\n",
    "\n",
    "Content:\n",
    "{context[:1000]}\n",
    "\"\"\"\n",
    "                result = llm_pipeline(prompt, max_new_tokens=1000, temperature=0.7)\n",
    "                if result:\n",
    "                    text = result[0]['generated_text']\n",
    "                    # Try to extract JSON array\n",
    "                    match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "                    if match:\n",
    "                        try:\n",
    "                            cards = ast.literal_eval(match.group(0))\n",
    "                            if isinstance(cards, list) and len(cards) > 0:\n",
    "                                return cards[:n]\n",
    "                        except:\n",
    "                            pass\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå RAG flashcard generation failed: {e}\")\n",
    "    \n",
    "    # Fallback flashcards\n",
    "    flashcard_pools = {\n",
    "        \"cudf\": [\n",
    "            {\"front\": \"What is cuDF?\", \"back\": \"cuDF is a GPU DataFrame library with pandas-like API.\"},\n",
    "            {\"front\": \"How to convert pandas to cuDF?\", \"back\": \"Use cudf.from_pandas(df)\"},\n",
    "            {\"front\": \"Main advantage of cuDF?\", \"back\": \"GPU acceleration for large dataset processing\"}\n",
    "        ],\n",
    "        \"pandas\": [\n",
    "            {\"front\": \"What is a DataFrame?\", \"back\": \"A 2D labeled data structure with columns of different types.\"},\n",
    "            {\"front\": \"How to read CSV in pandas?\", \"back\": \"pd.read_csv('filename.csv')\"},\n",
    "            {\"front\": \"How to select a column?\", \"back\": \"df['column_name'] or df.column_name\"}\n",
    "        ],\n",
    "        \"rapids\": [\n",
    "            {\"front\": \"What is RAPIDS?\", \"back\": \"A suite of GPU-accelerated data science libraries.\"},\n",
    "            {\"front\": \"Main RAPIDS libraries?\", \"back\": \"cuDF, cuML, cuGraph for DataFrames, ML, and graphs.\"},\n",
    "            {\"front\": \"RAPIDS vs traditional tools?\", \"back\": \"Same APIs but with GPU acceleration for massive speedups.\"}\n",
    "        ],\n",
    "        \"cuda\": [\n",
    "            {\"front\": \"What does CUDA stand for?\", \"back\": \"Compute Unified Device Architecture\"},\n",
    "            {\"front\": \"What is CUDA used for?\", \"back\": \"Parallel computing on NVIDIA GPUs\"},\n",
    "            {\"front\": \"CUDA vs CPU computing?\", \"back\": \"CUDA enables massive parallelization vs sequential CPU processing\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Get cards for topic, with fallback\n",
    "    cards = flashcard_pools.get(topic.lower(), [\n",
    "        {\"front\": f\"What is {topic}?\", \"back\": f\"{topic} is an important concept in data science.\"},\n",
    "        {\"front\": f\"Why learn {topic}?\", \"back\": f\"Understanding {topic} improves your data science skills.\"}\n",
    "    ])\n",
    "    \n",
    "    return cards[:n]\n",
    "\n",
    "def generate_quiz(topic, n=2):\n",
    "    \"\"\"Generate quiz questions\"\"\"\n",
    "    \n",
    "    # Quiz pools by topic\n",
    "    quiz_pools = {\n",
    "        \"pandas\": [\n",
    "            {\"question\": \"What is the main data structure in pandas?\", \"options\": [\"DataFrame\", \"Array\", \"List\", \"Dict\"], \"answer_idx\": 0},\n",
    "            {\"question\": \"How do you read a CSV file?\", \"options\": [\"pd.read_csv()\", \"pd.load()\", \"pd.import()\", \"pd.open()\"], \"answer_idx\": 0}\n",
    "        ],\n",
    "        \"cudf\": [\n",
    "            {\"question\": \"cuDF accelerates which library?\", \"options\": [\"NumPy\", \"Pandas\", \"SciPy\", \"Matplotlib\"], \"answer_idx\": 1},\n",
    "            {\"question\": \"cuDF runs on which hardware?\", \"options\": [\"CPU\", \"GPU\", \"TPU\", \"FPGA\"], \"answer_idx\": 1}\n",
    "        ],\n",
    "        \"rapids\": [\n",
    "            {\"question\": \"RAPIDS is developed by?\", \"options\": [\"Google\", \"Facebook\", \"NVIDIA\", \"Microsoft\"], \"answer_idx\": 2},\n",
    "            {\"question\": \"Main RAPIDS component for DataFrames?\", \"options\": [\"cuML\", \"cuDF\", \"cuGraph\", \"cuPy\"], \"answer_idx\": 1}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    questions = quiz_pools.get(topic.lower(), [\n",
    "        {\"question\": f\"What is {topic}?\", \"options\": [\"A tool\", \"A library\", \"A concept\", \"All of above\"], \"answer_idx\": 3},\n",
    "        {\"question\": f\"Why is {topic} important?\", \"options\": [\"Performance\", \"Efficiency\", \"Scale\", \"All of above\"], \"answer_idx\": 3}\n",
    "    ])\n",
    "    \n",
    "    return random.sample(questions, min(n, len(questions)))\n",
    "\n",
    "\n",
    "def generate_coding_puzzle(difficulty=\"Beginner\"):\n",
    "    \"\"\"Generate coding puzzles for pandas to cuDF conversion\"\"\"\n",
    "    \n",
    "    puzzles = {\n",
    "        \"Beginner\": {\n",
    "            \"description\": \"Convert basic DataFrame creation from pandas to cuDF\",\n",
    "            \"cpu_code\": \"import pandas as pd\\ndf = pd.DataFrame({'a': [1, 2, 3]})\\nprint(df)\",\n",
    "            \"gpu_code\": \"import cudf\\ndf = cudf.DataFrame({'a': [1, 2, 3]})\\nprint(df)\",\n",
    "            \"cpu_time\": 1.2,\n",
    "            \"gpu_time\": 0.2\n",
    "        },\n",
    "        \"Intermediate\": {\n",
    "            \"description\": \"Convert groupby operation from pandas to cuDF\",\n",
    "            \"cpu_code\": \"import pandas as pd\\ndf = pd.DataFrame({'group': ['A', 'B', 'A'], 'value': [1, 2, 3]})\\nresult = df.groupby('group').sum()\",\n",
    "            \"gpu_code\": \"import cudf\\ndf = cudf.DataFrame({'group': ['A', 'B', 'A'], 'value': [1, 2, 3]})\\nresult = df.groupby('group').sum()\",\n",
    "            \"cpu_time\": 3.2,\n",
    "            \"gpu_time\": 0.5\n",
    "        },\n",
    "        \"Advanced\": {\n",
    "            \"description\": \"Convert complex aggregation from pandas to cuDF\",\n",
    "            \"cpu_code\": \"import pandas as pd\\ndf = pd.DataFrame({'cat': ['A', 'B'], 'val1': [1, 2], 'val2': [3, 4]})\\nresult = df.groupby('cat').agg({'val1': 'sum', 'val2': 'mean'})\",\n",
    "            \"gpu_code\": \"import cudf\\ndf = cudf.DataFrame({'cat': ['A', 'B'], 'val1': [1, 2], 'val2': [3, 4]})\\nresult = df.groupby('cat').agg({'val1': 'sum', 'val2': 'mean'})\",\n",
    "            \"cpu_time\": 4.2,\n",
    "            \"gpu_time\": 0.6\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return puzzles.get(difficulty, puzzles[\"Beginner\"])\n",
    "\n",
    "\n",
    "def create_speedup_visualization(cpu_time, gpu_time, speedup):\n",
    "    \"\"\"Create a visual ASCII-style chart for speedup comparison\"\"\"\n",
    "    max_width = 50\n",
    "    cpu_bar = \"‚ñà\" * max_width\n",
    "    gpu_bar = \"‚ñà\" * max(1, int(max_width / speedup))\n",
    "    \n",
    "    chart = f\"\"\"\n",
    "```\n",
    "PERFORMANCE COMPARISON CHART\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "üñ•Ô∏è  CPU (pandas):  {cpu_bar} {cpu_time}s\n",
    "üöÄ  GPU (cuDF):    {gpu_bar} {gpu_time}s\n",
    "\n",
    "SPEEDUP: {speedup}x faster! üî•\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "```\n",
    "\"\"\"\n",
    "    return chart\n",
    "\n",
    "\n",
    "# === GRADIO INTERFACE ===\n",
    "def create_ai_tutor():\n",
    "    \"\"\"Create the AI Tutor Gradio interface\"\"\"\n",
    "    \n",
    "    with gr.Blocks(theme=gr.themes.Soft(), title=\"üß† AI Tutor\") as app:\n",
    "        \n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        <div style='text-align:center; background:linear-gradient(90deg,#fff1c1,#c1e7ff,#e1ffc1); border-radius:15px; padding:15px; margin-bottom:20px; color:#000;'>\n",
    "          <h1 style='color:#000; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);'>üß† AI Tutor - Complete Learning System</h1>\n",
    "          <h3 style='color:#000; font-weight:700; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);'>üìö Learning Mode ‚Ä¢ üéì Socratic Tutoring ‚Ä¢ üéÆ Gamified Practice</h3>\n",
    "          <p style='color:#000; font-weight:600; text-shadow: 1px 1px 2px rgba(0,0,0,0.1);'><b>üìö Structured Learning Paths ‚Ä¢ ü§î Guided Discovery ‚Ä¢ üéØ Interactive Practice</b></p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "        \n",
    "        # Add custom CSS for better styling\n",
    "        app.css = \"\"\"\n",
    "        .learning-topic-btn {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;\n",
    "            color: white !important;\n",
    "            border: none !important;\n",
    "            border-radius: 12px !important;\n",
    "            padding: 15px !important;\n",
    "            margin: 8px !important;\n",
    "            font-weight: 600 !important;\n",
    "            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4) !important;\n",
    "            transition: all 0.3s ease !important;\n",
    "            min-height: 80px !important;\n",
    "            font-size: 14px !important;\n",
    "        }\n",
    "        .learning-topic-btn:hover {\n",
    "            transform: translateY(-2px) !important;\n",
    "            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6) !important;\n",
    "        }\n",
    "        \n",
    "        /* Make tab headers more prominent */\n",
    "        .gradio-tab-nav {\n",
    "            background: linear-gradient(90deg, #f8fafc, #e2e8f0) !important;\n",
    "            border-radius: 8px !important;\n",
    "            padding: 8px !important;\n",
    "            margin-bottom: 20px !important;\n",
    "            box-shadow: 0 2px 8px rgba(0,0,0,0.1) !important;\n",
    "        }\n",
    "        \n",
    "        .gradio-tab-nav .tab-nav {\n",
    "            background: linear-gradient(135deg, #4f46e5, #7c3aed) !important;\n",
    "            color: white !important;\n",
    "            font-weight: 700 !important;\n",
    "            font-size: 16px !important;\n",
    "            border-radius: 6px !important;\n",
    "            margin: 4px !important;\n",
    "            padding: 12px 20px !important;\n",
    "            box-shadow: 0 2px 4px rgba(79, 70, 229, 0.3) !important;\n",
    "            border: none !important;\n",
    "        }\n",
    "        \n",
    "        .gradio-tab-nav .tab-nav:hover {\n",
    "            background: linear-gradient(135deg, #6366f1, #8b5cf6) !important;\n",
    "            transform: translateY(-1px) !important;\n",
    "            box-shadow: 0 4px 8px rgba(79, 70, 229, 0.4) !important;\n",
    "        }\n",
    "        \n",
    "        .gradio-tab-nav .tab-nav.selected {\n",
    "            background: linear-gradient(135deg, #059669, #0d9488) !important;\n",
    "            box-shadow: 0 4px 12px rgba(5, 150, 105, 0.4) !important;\n",
    "        }\n",
    "        \n",
    "        /* Benchmark button styling */\n",
    "        .benchmark-btn {\n",
    "            background: linear-gradient(135deg, #ff6b35, #f7931e) !important;\n",
    "            color: white !important;\n",
    "            font-weight: 700 !important;\n",
    "            font-size: 18px !important;\n",
    "            padding: 15px 30px !important;\n",
    "            border-radius: 12px !important;\n",
    "            border: none !important;\n",
    "            box-shadow: 0 4px 15px rgba(255, 107, 53, 0.4) !important;\n",
    "            transition: all 0.3s ease !important;\n",
    "            text-transform: uppercase !important;\n",
    "            letter-spacing: 1px !important;\n",
    "        }\n",
    "        \n",
    "        .benchmark-btn:hover {\n",
    "            background: linear-gradient(135deg, #ff8c5a, #ffb347) !important;\n",
    "            transform: translateY(-2px) !important;\n",
    "            box-shadow: 0 6px 20px rgba(255, 107, 53, 0.6) !important;\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            \n",
    "            # === LEARNING MODE ===\n",
    "            with gr.Tab(\"üìö Learning Mode\"):\n",
    "                gr.Markdown(\"### üéØ **Choose a Learning Path - From Beginner to Advanced**\")\n",
    "                gr.Markdown(\"*Select any topic below to get a structured learning journey with resources and related topics*\")\n",
    "                \n",
    "                # Learning topics with structured paths\n",
    "                LEARNING_TOPICS = {\n",
    "                    \"Python for Data Science\": {\n",
    "                        \"description\": \"Master Python fundamentals and data science libraries\",\n",
    "                        \"level\": \"üü¢ Beginner to Intermediate\",\n",
    "                        \"duration\": \"4-6 weeks\",\n",
    "                        \"prerequisites\": \"Basic programming knowledge\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **Python Basics** - Variables, data types, control structures\n",
    "2. **NumPy** - Numerical computing and arrays\n",
    "3. **Pandas** - Data manipulation and analysis\n",
    "4. **Matplotlib/Seaborn** - Data visualization\n",
    "5. **Jupyter Notebooks** - Interactive development\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Python.org Tutorial](https://docs.python.org/3/tutorial/)\n",
    "‚Ä¢ [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "‚Ä¢ [NumPy User Guide](https://numpy.org/doc/stable/user/)\n",
    "\n",
    "**üöÄ Next Steps:** Machine Learning Fundamentals, Data Visualization\n",
    "\"\"\",\n",
    "                        \"related\": [\"Machine Learning Fundamentals\", \"Data Visualization\", \"Pandas Deep Dive\"]\n",
    "                    },\n",
    "                    \"Pandas Deep Dive\": {\n",
    "                        \"description\": \"Master DataFrame operations, data cleaning, and advanced pandas techniques\",\n",
    "                        \"level\": \"üü° Intermediate\",\n",
    "                        \"duration\": \"3-4 weeks\", \n",
    "                        \"prerequisites\": \"Python basics, basic pandas knowledge\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **DataFrame Mastery** - Creation, indexing, selection\n",
    "2. **Data Cleaning** - Missing values, duplicates, data types\n",
    "3. **GroupBy Operations** - Aggregation and transformation\n",
    "4. **Merging & Joining** - Combining datasets\n",
    "5. **Performance Optimization** - Efficient pandas operations\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "‚Ä¢ [Pandas Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html)\n",
    "‚Ä¢ [10 Minutes to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "\n",
    "**üöÄ Next Steps:** GPU Acceleration with cuDF, Time Series Analysis\n",
    "\"\"\",\n",
    "                        \"related\": [\"GPU Acceleration with cuDF\", \"Data Visualization\", \"Time Series Analysis\"]\n",
    "                    },\n",
    "                    \"GPU Acceleration with cuDF\": {\n",
    "                        \"description\": \"Learn GPU-accelerated data processing with RAPIDS cuDF\",\n",
    "                        \"level\": \"üî¥ Advanced\",\n",
    "                        \"duration\": \"2-3 weeks\",\n",
    "                        \"prerequisites\": \"Strong pandas knowledge, basic GPU concepts\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **GPU Computing Basics** - Understanding parallel processing\n",
    "2. **cuDF Introduction** - GPU DataFrames and basic operations\n",
    "3. **Migration from Pandas** - Converting existing code\n",
    "4. **Performance Optimization** - Memory management and best practices\n",
    "5. **Advanced Operations** - Complex aggregations and joins\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [RAPIDS cuDF Documentation](https://docs.rapids.ai/api/cudf/stable/)\n",
    "‚Ä¢ [cuDF User Guide](https://docs.rapids.ai/api/cudf/stable/user_guide/)\n",
    "‚Ä¢ [RAPIDS Getting Started](https://rapids.ai/start.html)\n",
    "\n",
    "**üöÄ Next Steps:** RAPIDS Ecosystem, Machine Learning with cuML\n",
    "\"\"\",\n",
    "                        \"related\": [\"RAPIDS Ecosystem\", \"CUDA Programming\", \"High-Performance Computing\"]\n",
    "                    },\n",
    "                    \"Machine Learning Fundamentals\": {\n",
    "                        \"description\": \"Core ML concepts, algorithms, and scikit-learn implementation\",\n",
    "                        \"level\": \"üü° Intermediate\",\n",
    "                        \"duration\": \"6-8 weeks\",\n",
    "                        \"prerequisites\": \"Python, pandas, basic statistics\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **ML Concepts** - Supervised, unsupervised, reinforcement learning\n",
    "2. **Data Preprocessing** - Feature scaling, encoding, train-test splits\n",
    "3. **Regression Algorithms** - Linear, polynomial, regularization\n",
    "4. **Classification** - Logistic regression, decision trees, SVM\n",
    "5. **Model Evaluation** - Cross-validation, metrics, hyperparameter tuning\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Scikit-learn Documentation](https://scikit-learn.org/stable/)\n",
    "‚Ä¢ [Machine Learning Course by Andrew Ng](https://www.coursera.org/learn/machine-learning)\n",
    "‚Ä¢ [Hands-On Machine Learning](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "**üöÄ Next Steps:** Deep Learning, GPU-Accelerated ML with cuML\n",
    "\"\"\",\n",
    "                        \"related\": [\"Deep Learning Fundamentals\", \"GPU-Accelerated ML\", \"Data Science Projects\"]\n",
    "                    },\n",
    "                    \"RAPIDS Ecosystem\": {\n",
    "                        \"description\": \"Complete RAPIDS suite: cuDF, cuML, cuGraph for end-to-end GPU data science\",\n",
    "                        \"level\": \"üî¥ Advanced\",\n",
    "                        \"duration\": \"4-5 weeks\",\n",
    "                        \"prerequisites\": \"GPU computing basics, pandas, scikit-learn\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **RAPIDS Overview** - cuDF, cuML, cuGraph, cuPy integration\n",
    "2. **cuDF Mastery** - Advanced DataFrame operations on GPU\n",
    "3. **cuML for ML** - GPU-accelerated machine learning algorithms\n",
    "4. **cuGraph** - Graph analytics and network analysis\n",
    "5. **End-to-End Workflows** - Complete GPU data science pipelines\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [RAPIDS.ai Main Site](https://rapids.ai/)\n",
    "‚Ä¢ [RAPIDS Documentation](https://docs.rapids.ai/)\n",
    "‚Ä¢ [RAPIDS Community](https://github.com/rapidsai)\n",
    "\n",
    "**üöÄ Next Steps:** Production Deployment, Multi-GPU Computing\n",
    "\"\"\",\n",
    "                        \"related\": [\"Multi-GPU Computing\", \"Production ML Systems\", \"CUDA Programming\"]\n",
    "                    },\n",
    "                    \"Data Visualization\": {\n",
    "                        \"description\": \"Create compelling data visualizations with matplotlib, seaborn, and plotly\",\n",
    "                        \"level\": \"üü¢ Beginner to Intermediate\",\n",
    "                        \"duration\": \"3-4 weeks\",\n",
    "                        \"prerequisites\": \"Python basics, pandas fundamentals\",\n",
    "                        \"content\": \"\"\"\n",
    "**üìñ Learning Path:**\n",
    "1. **Matplotlib Basics** - Plots, figures, axes, customization\n",
    "2. **Seaborn for Statistics** - Statistical plots and themes\n",
    "3. **Interactive Plotly** - Dynamic and web-ready visualizations\n",
    "4. **Advanced Techniques** - Subplots, animations, custom plots\n",
    "5. **Dashboard Creation** - Streamlit, Dash for interactive apps\n",
    "\n",
    "**üîó Key Resources:**\n",
    "‚Ä¢ [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "‚Ä¢ [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)\n",
    "‚Ä¢ [Plotly Python Guide](https://plotly.com/python/)\n",
    "\n",
    "**üöÄ Next Steps:** Interactive Dashboards, Business Intelligence\n",
    "\"\"\",\n",
    "                        \"related\": [\"Interactive Dashboards\", \"Business Intelligence\", \"Web Development for Data Science\"]\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Topic selection buttons\n",
    "                with gr.Row():\n",
    "                    topic_buttons = []\n",
    "                    for i, (topic_name, topic_info) in enumerate(LEARNING_TOPICS.items()):\n",
    "                        if i % 2 == 0 and i > 0:\n",
    "                            # Start new row every 2 buttons\n",
    "                            pass\n",
    "                        \n",
    "                        color_map = {\n",
    "                            \"üü¢\": \"#dcfce7\",  # Green for beginner\n",
    "                            \"üü°\": \"#fef3c7\",  # Yellow for intermediate  \n",
    "                            \"üî¥\": \"#fee2e2\"   # Red for advanced\n",
    "                        }\n",
    "                        level_color = color_map.get(topic_info[\"level\"][0], \"#f3f4f6\")\n",
    "                        \n",
    "                        btn = gr.Button(\n",
    "                            f\"{topic_name}\\n{topic_info['level']}\",\n",
    "                            elem_classes=\"learning-topic-btn\",\n",
    "                            size=\"lg\"\n",
    "                        )\n",
    "                        topic_buttons.append((btn, topic_name))\n",
    "                \n",
    "                # Learning content display\n",
    "                learning_content = gr.Markdown()\n",
    "                related_topics = gr.Markdown()\n",
    "                \n",
    "                def show_learning_content(topic_name):\n",
    "                    if topic_name not in LEARNING_TOPICS:\n",
    "                        return \"Topic not found!\", \"\"\n",
    "                    \n",
    "                    topic = LEARNING_TOPICS[topic_name]\n",
    "                    \n",
    "                    content = f\"\"\"\n",
    "# üìö {topic_name}\n",
    "\n",
    "**üìã Description:** {topic['description']}\n",
    "\n",
    "**üìä Level:** {topic['level']} | **‚è±Ô∏è Duration:** {topic['duration']} | **üìö Prerequisites:** {topic['prerequisites']}\n",
    "\n",
    "---\n",
    "\n",
    "{topic['content']}\n",
    "\n",
    "---\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    related = f\"\"\"\n",
    "### üîó **Related Learning Paths:**\n",
    "{' ‚Ä¢ '.join([f\"**{rel}**\" for rel in topic['related']])}\n",
    "\n",
    "üí° *Click any topic above to explore these related learning paths!*\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    return content, related\n",
    "                \n",
    "                # Connect buttons to content display\n",
    "                for btn, topic_name in topic_buttons:\n",
    "                    btn.click(\n",
    "                        lambda tn=topic_name: show_learning_content(tn),\n",
    "                        outputs=[learning_content, related_topics]\n",
    "                    )\n",
    "                \n",
    "                # Search functionality\n",
    "                with gr.Row():\n",
    "                    search_topic = gr.Textbox(\n",
    "                        label=\"üîç Search for specific topics\",\n",
    "                        placeholder=\"e.g., neural networks, time series, NLP, computer vision...\"\n",
    "                    )\n",
    "                    search_btn = gr.Button(\"Search Learning Resources\")\n",
    "                \n",
    "                search_results = gr.Markdown()\n",
    "                \n",
    "                def search_learning_resources(query):\n",
    "                    if not query:\n",
    "                        return \"Please enter a search term!\"\n",
    "                    \n",
    "                    # Simulate search results with relevant resources\n",
    "                    query_lower = query.lower()\n",
    "                    \n",
    "                    results = []\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['neural', 'deep', 'cnn', 'rnn', 'transformer']):\n",
    "                        results.append(\"\"\"\n",
    "**üß† Deep Learning Resources:**\n",
    "‚Ä¢ [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n",
    "‚Ä¢ [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "‚Ä¢ [TensorFlow Guide](https://www.tensorflow.org/guide)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['nlp', 'text', 'language', 'sentiment']):\n",
    "                        results.append(\"\"\"\n",
    "**üìù Natural Language Processing:**\n",
    "‚Ä¢ [NLTK Documentation](https://www.nltk.org/)\n",
    "‚Ä¢ [spaCy Course](https://course.spacy.io/)\n",
    "‚Ä¢ [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['time series', 'forecasting', 'temporal']):\n",
    "                        results.append(\"\"\"\n",
    "**üìà Time Series Analysis:**\n",
    "‚Ä¢ [Time Series Analysis Guide](https://www.statsmodels.org/stable/tsa.html)\n",
    "‚Ä¢ [Prophet Forecasting](https://facebook.github.io/prophet/)\n",
    "‚Ä¢ [Time Series with Python](https://github.com/marcopeix/TimeSeriesForecastingInPython)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if any(term in query_lower for term in ['computer vision', 'image', 'cv', 'opencv']):\n",
    "                        results.append(\"\"\"\n",
    "**üëÅÔ∏è Computer Vision:**\n",
    "‚Ä¢ [OpenCV Tutorials](https://docs.opencv.org/master/d9/df8/tutorial_root.html)\n",
    "‚Ä¢ [Computer Vision Course](https://www.coursera.org/learn/convolutional-neural-networks)\n",
    "‚Ä¢ [Fast.ai Practical Deep Learning](https://course.fast.ai/)\n",
    "\"\"\")\n",
    "                    \n",
    "                    if not results:\n",
    "                        results.append(f\"\"\"\n",
    "**üîç Search Results for \"{query}\":**\n",
    "\n",
    "*No specific resources found. Try these general resources:*\n",
    "‚Ä¢ [Kaggle Learn](https://www.kaggle.com/learn) - Free micro-courses\n",
    "‚Ä¢ [Coursera Data Science](https://www.coursera.org/browse/data-science)\n",
    "‚Ä¢ [edX MIT Data Science](https://www.edx.org/course/introduction-to-computational-thinking-and-data-science)\n",
    "\n",
    "*Or refine your search with terms like: machine learning, deep learning, NLP, computer vision, time series*\n",
    "\"\"\")\n",
    "                    \n",
    "                    return \"\\n\".join(results)\n",
    "                \n",
    "                search_btn.click(search_learning_resources, inputs=search_topic, outputs=search_results)\n",
    "\n",
    "            # === TUTOR MODE ===\n",
    "            with gr.Tab(\"üéì Tutor Mode\"):\n",
    "                gr.Markdown(\"### ü§î **Socratic Learning - Ask Questions, Get Guided Answers**\")\n",
    "                gr.Markdown(\"*I won't give you direct answers, but I'll guide you to discover the knowledge yourself!*\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        tutor_query = gr.Textbox(\n",
    "                            label=\"Ask about Data Science, GPU Computing, or type 'docs' for documentation\",\n",
    "                            placeholder=\"e.g., What is RAPIDS? How does cuDF work? Why use GPU for data science?\",\n",
    "                            lines=2\n",
    "                        )\n",
    "                        tutor_btn = gr.Button(\"üöÄ Get Socratic Guidance\", variant=\"primary\")\n",
    "                    \n",
    "                    with gr.Column(scale=1):\n",
    "                        quote_display = gr.Markdown(\"üí° *Ready to discover knowledge!*\")\n",
    "                \n",
    "                tutor_answer = gr.Markdown(label=\"ü§î Socratic Guidance\")\n",
    "                tutor_source = gr.Markdown(label=\"üìö Source\")\n",
    "                \n",
    "                # Quick question suggestions\n",
    "                with gr.Row():\n",
    "                    quick_questions = [\n",
    "                        \"Why use GPUs for data science?\",\n",
    "                        \"How does parallel processing work?\", \n",
    "                        \"What makes cuDF faster than pandas?\",\n",
    "                        \"When should I use machine learning?\"\n",
    "                    ]\n",
    "                    for i, question in enumerate(quick_questions):\n",
    "                        if i % 2 == 0:\n",
    "                            with gr.Column():\n",
    "                                pass\n",
    "                        quick_btn = gr.Button(f\"üí≠ {question}\", size=\"sm\")\n",
    "                        quick_btn.click(lambda q=question: q, outputs=tutor_query)\n",
    "                \n",
    "                def handle_tutor_query_streaming(query):\n",
    "                    if not query:\n",
    "                        yield \"Please ask a question!\", \"AI Tutor\", \"üí° *Ready to help!*\"\n",
    "                        return\n",
    "                    \n",
    "                    # Handle docs command\n",
    "                    if query.lower().strip() in ['docs', 'documentation', 'help']:\n",
    "                        docs_info = \"\"\"üìö **Available Documentation:**\n",
    "‚Ä¢ **Core Libraries:** pandas, cuDF, RAPIDS, scikit-learn\n",
    "‚Ä¢ **GPU Computing:** CUDA, NVIDIA resources  \n",
    "‚Ä¢ **Data Science:** Comprehensive guides and tutorials\n",
    "*Ask any topic-specific question to get relevant documentation!*\"\"\"\n",
    "                        yield docs_info, \"Documentation System\", \"üìñ *Knowledge at your fingertips!*\"\n",
    "                        return\n",
    "                    \n",
    "                    # Generate contextual quote\n",
    "                    quote = generate_contextual_quote(extract_topic_from_query(query), query)\n",
    "                    \n",
    "                    # Stream the response\n",
    "                    accumulated_response = \"\"\n",
    "                    for token in smart_tutor_answer_streaming(query):\n",
    "                        accumulated_response += token\n",
    "                        yield accumulated_response, f\"**Source:** AI Tutor (Streaming)\", f\"üåü {quote}\"\n",
    "                \n",
    "                # Update the click handlers to use streaming\n",
    "                tutor_btn.click(\n",
    "                    handle_tutor_query_streaming, \n",
    "                    inputs=tutor_query, \n",
    "                    outputs=[tutor_answer, tutor_source, quote_display],\n",
    "                    show_progress=True\n",
    "                )\n",
    "                tutor_query.submit(\n",
    "                    handle_tutor_query_streaming, \n",
    "                    inputs=tutor_query, \n",
    "                    outputs=[tutor_answer, tutor_source, quote_display],\n",
    "                    show_progress=True\n",
    "                )\n",
    "            \n",
    "            # === GAME MODE ===\n",
    "            with gr.Tab(\"üéÆ Game Mode\"):\n",
    "                user_state = SimpleNamespace(points=0)\n",
    "                \n",
    "                def update_progress():\n",
    "                    level = user_state.points // 20 + 1\n",
    "                    bar_width = min((user_state.points % 20) * 5, 100)\n",
    "                    return f\"\"\"<h3>üèÜ Level {level} | üî• {user_state.points} XP</h3>\n",
    "                    <div style='background: #e5e7eb; height: 20px; border-radius: 10px;'>\n",
    "                        <div style='background: linear-gradient(90deg, #4ade80, #22c55e); height: 20px; width: {bar_width}%; border-radius: 10px;'></div>\n",
    "                    </div>\"\"\"\n",
    "                \n",
    "                progress = gr.HTML(update_progress())\n",
    "                \n",
    "                #with gr.Tabs():\n",
    "                    \n",
    "                # === FLASHCARDS ===\n",
    "                with gr.Tabs():\n",
    "                    with gr.Tab(\"üÉè Flashcards\"):\n",
    "                        with gr.Tab(\"üîß Generate Flashcards\"):\n",
    "                            gen_query = gr.Textbox(label=\"Enter a topic (e.g., cuDF, CUDA, RAPIDS)\", placeholder=\"cuDF vs pandas?\")\n",
    "                            gen_btn = gr.Button(\"‚ú® Generate Flashcards\")\n",
    "                \n",
    "                            card_front = gr.HTML()\n",
    "                            card_back = gr.HTML(visible=False)\n",
    "                            instruction_msg = gr.Markdown(\"üëÜ Choose a card to flip!\", visible=False)\n",
    "                            feedback_msg = gr.Markdown(visible=False)\n",
    "                \n",
    "                            with gr.Row():\n",
    "                                flip_btn1 = gr.Button(\"‚úÖ Flip Card 1\", visible=False)\n",
    "                                flip_btn2 = gr.Button(\"üîÑ Flip Card 2\", visible=False)\n",
    "                \n",
    "                            reset_btn = gr.Button(\"üîÅ Reset\", visible=False)\n",
    "                \n",
    "                            flash_idx = gr.State(0)\n",
    "                            show_answer = gr.State(False)\n",
    "                            flashcards_state = gr.State([])\n",
    "                \n",
    "                            import ast\n",
    "                            import random\n",
    "                            import re\n",
    "                            def generate_flashcards_from_rag(query, index, chunks, k=3):\n",
    "                                context, source = query_rag(query, index, chunks)\n",
    "                \n",
    "                                if not context or len(context.strip()) < 100:\n",
    "                                    return [{\"front\": \"Could not find content\", \"back\": \"Try another topic.\", \"wrong\": \"This is a placeholder wrong answer.\"}]\n",
    "                \n",
    "                                prompt = f\"\"\"\n",
    "                            You're an expert AI tutor. From the following technical text, generate exactly {k} multiple-choice style flashcards in **valid JSON**.\n",
    "                            \n",
    "                            Each flashcard must be a dictionary with:\n",
    "                            - \"front\": the question\n",
    "                            - \"back\": the correct answer\n",
    "                            - \"wrong\": a wrong but plausible distractor\n",
    "                            \n",
    "                            Example format:\n",
    "                            \n",
    "                            [\n",
    "                              {{\n",
    "                                \"front\": \"Why is cuDF faster than pandas?\",\n",
    "                                \"back\": \"It uses GPU acceleration.\",\n",
    "                                \"wrong\": \"Because it uses more RAM.\"\n",
    "                              }},\n",
    "                              ...\n",
    "                            ]\n",
    "                            \n",
    "                            DO NOT add explanation, markdown, or anything else. Just output a valid JSON list of 3 objects.\n",
    "                            \n",
    "                            Text:\n",
    "                            ------------------------\n",
    "                            {context}\n",
    "                            ------------------------\n",
    "                            \"\"\"\n",
    "                            \n",
    "                                try:\n",
    "                                    raw = llm_pipeline(prompt)[0]['generated_text']\n",
    "                            \n",
    "                                    # DEBUG: Log raw output when things fail\n",
    "                                    print(\"\\nüîç RAW OUTPUT FROM MODEL:\\n\", raw[:500], \"...\\n\")\n",
    "                            \n",
    "                                    json_text = re.search(r'\\[\\s*{.*?}\\s*\\]', raw, re.DOTALL)\n",
    "                                    if json_text:\n",
    "                                        return ast.literal_eval(json_text.group())\n",
    "                                except Exception as e:\n",
    "                                    print(f\"‚ùå Flashcard generation failed: {e}\")\n",
    "                \n",
    "                                return [{\"front\": \"Could not parse flashcards\", \"back\": \"\", \"wrong\": \"\"}]\n",
    "                \n",
    "                            def style_card(text):\n",
    "                                return f\"\"\"\n",
    "                <div style='font-size:18px;padding:10px;border:2px solid #ccc;border-radius:10px;background:#fefefe;transition:transform 0.5s;transform-style: preserve-3d;'>\n",
    "                {text}\n",
    "                </div>\"\"\"\n",
    "                \n",
    "                            def flashcard_from_query(topic, idx):\n",
    "                                if not topic or topic.strip() == \"\":\n",
    "                                    return (\n",
    "                                            style_card(\"**Q:** Please enter a topic to generate flashcards.\"),\n",
    "                                            \"\",\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=False),\n",
    "                                            gr.update(visible=True, value=\"<span style='color:red; font-size:18px;'>‚ùó Please enter a topic to begin.</span>\"),\n",
    "                                            idx, False, [], True, False ) # empty flashcard state\n",
    "                                cards = generate_flashcards_from_rag(topic, rag_index, rag_chunks)\n",
    "                                #cards = generate_flashcards_from_rag(topic)\n",
    "                                idx = (idx + 1) % len(cards)\n",
    "                                card = cards[idx]\n",
    "                                buttons = [(\"‚úÖ Flip Card üßê \", True), (\"‚úÖ Flip Card ü§ì\", False)]\n",
    "                                random.shuffle(buttons)\n",
    "                                return (\n",
    "                                    style_card(f\"**Q:** {card['front']}\"),\n",
    "                                    \"\",\n",
    "                                    gr.update(visible=True, value=buttons[0][0]),\n",
    "                                    gr.update(visible=True, value=buttons[1][0]),\n",
    "                                    gr.update(visible=True),\n",
    "                                    gr.update(visible=False, value=\"\"),\n",
    "                                    gr.update(visible=True, value=\"<span style='color:green; font-size:18px;'>üëÜ Choose a card to flip!</span>\"),\n",
    "                                    idx, False, cards, buttons[0][1], buttons[1][1]\n",
    "                                )\n",
    "                \n",
    "                            def flip_card(idx, cards, correct):\n",
    "                                card = cards[idx % len(cards)]\n",
    "                                front_html = style_card(f\"**Q:** {card['front']}\")\n",
    "    \n",
    "                                if correct:\n",
    "                                    back_html = style_card(f\"‚úÖ Answer: {card['back']}\")\n",
    "                                else:\n",
    "                                    back_html = gr.update(visible=False)  # hide back card for wrong pick\n",
    "                                wrong_msgs = [\n",
    "                                    \"üö´ Not quite! As they say, 'Data is the new oil'‚Äîbut you've hit a dry well. Try the other card!\",\n",
    "                                    \"üí° 'In God we trust, all others bring data.' Sadly, this card didn't bring it. Flip the other one!\",\n",
    "                                    \"üß† Oops! ‚ÄòThe greatest value of a picture is when it forces us to notice what we never expected to see.‚Äô You missed it‚Äîcheck the other card.\",\n",
    "                                    \"‚öôÔ∏è 'GPU acceleration turns hours into seconds' ‚Äî but this pick cost you a moment. Try the other one!\",\n",
    "                                    \"üìä 'Torture the data long enough, and it will confess to anything.' This card stayed silent. Flip the other!\",\n",
    "                                    \"‚ùå Wrong Pick! 'Without data, you're just another person with an opinion.' Try the other card for some real answers!\",\n",
    "                                    \"üöÄ 'GPUs don‚Äôt guess‚Äîthey compute at scale.' Your guess here missed. Go try the other card!\",\n",
    "                                    \"üîÑ Oops! You‚Äôve hit a cold cache. Try the other card for a GPU-hot answer.\",\n",
    "                                    \"üí≠ 'Data science is the art of turning data into insight'‚Äîbut this card had none. Flip the other one!\"\n",
    "                                ]\n",
    "                                msg = \"‚úÖ Nailed it! Great job.\" if correct else random.choice(wrong_msgs)\n",
    "                            \n",
    "                                return (\n",
    "                                    front_html,\n",
    "                                    back_html,\n",
    "                                    gr.update(visible=True),\n",
    "                                    gr.update(visible=True, value=f\"<span style='font-size:18px; color:#b00020;'>{msg}</span>\"),      # ‚úÖ THIS is the fix\n",
    "                                    gr.update(visible=False),                # hide instruction\n",
    "                                    idx,\n",
    "                                    True,\n",
    "                                    cards\n",
    "                                )            \n",
    "                            btn1_correct = gr.State(True)\n",
    "                            btn2_correct = gr.State(False)\n",
    "                \n",
    "                            gen_btn.click(\n",
    "                                flashcard_from_query,\n",
    "                                inputs=[gen_query, flash_idx],\n",
    "                                outputs=[card_front, card_back, flip_btn1, flip_btn2, reset_btn, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state, btn1_correct, btn2_correct]\n",
    "                            )\n",
    "                \n",
    "                            flip_btn1.click(\n",
    "                                flip_card,\n",
    "                                inputs=[flash_idx, flashcards_state, btn1_correct],\n",
    "                                outputs=[card_front, card_back, card_back, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state]\n",
    "                            )\n",
    "                \n",
    "                            flip_btn2.click(\n",
    "                                flip_card,\n",
    "                                inputs=[flash_idx, flashcards_state, btn2_correct],\n",
    "                                outputs=[card_front, card_back, card_back, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state]\n",
    "                            )\n",
    "                \n",
    "                            reset_btn.click(\n",
    "                                flashcard_from_query,\n",
    "                                inputs=[gen_query, flash_idx],\n",
    "                                outputs=[card_front, card_back, flip_btn1, flip_btn2, reset_btn, feedback_msg, instruction_msg, flash_idx, show_answer, flashcards_state, btn1_correct, btn2_correct]\n",
    "                            )\n",
    "                        \n",
    "                    # === QUIZ ===\n",
    "                    with gr.Tab(\"‚ùì Quiz\"):\n",
    "                        quiz_topic = gr.Textbox(label=\"Quiz Topic\", placeholder=\"e.g., pandas, cuDF, RAPIDS\")\n",
    "                        gen_quiz_btn = gr.Button(\"üß† Generate Quiz\")\n",
    "                        \n",
    "                        quiz_q1 = gr.Markdown()\n",
    "                        quiz_r1 = gr.Radio(choices=[], label=\"Answer:\", visible=False)\n",
    "                        quiz_q2 = gr.Markdown()\n",
    "                        quiz_r2 = gr.Radio(choices=[], label=\"Answer:\", visible=False)\n",
    "                        \n",
    "                        submit_btn = gr.Button(\"Submit Quiz\", visible=False)\n",
    "                        quiz_result = gr.Markdown()\n",
    "                        quiz_state = gr.State([])\n",
    "                        \n",
    "                        def generate_quiz_questions(topic):\n",
    "                            if not topic:\n",
    "                                return \"Enter a topic!\", gr.update(visible=False), \"\", gr.update(visible=False), gr.update(visible=False), []\n",
    "                            \n",
    "                            questions = generate_quiz(topic, 2)\n",
    "                            return (\n",
    "                                f\"**Q1:** {questions[0]['question']}\",\n",
    "                                gr.update(choices=questions[0]['options'], visible=True, value=None),\n",
    "                                f\"**Q2:** {questions[1]['question']}\",\n",
    "                                gr.update(choices=questions[1]['options'], visible=True, value=None),\n",
    "                                gr.update(visible=True),\n",
    "                                questions\n",
    "                            )\n",
    "                        \n",
    "                        def evaluate_quiz(ans1, ans2, questions):\n",
    "                            if not questions:\n",
    "                                return \"Quiz not loaded!\", update_progress()\n",
    "                            \n",
    "                            correct = 0\n",
    "                            result_parts = []\n",
    "                            \n",
    "                            # Check Q1\n",
    "                            correct_ans1 = questions[0]['options'][questions[0]['answer_idx']]\n",
    "                            if ans1 and ans1 == correct_ans1:\n",
    "                                correct += 1\n",
    "                                result_parts.append(f\"‚úÖ **Q1:** Correct! ({ans1})\")\n",
    "                            else:\n",
    "                                result_parts.append(f\"‚ùå **Q1:** Wrong. You answered: {ans1 or 'None'}\")\n",
    "                                result_parts.append(f\"    üí° **Correct answer:** {correct_ans1}\")\n",
    "                            \n",
    "                            # Check Q2  \n",
    "                            correct_ans2 = questions[1]['options'][questions[1]['answer_idx']]\n",
    "                            if ans2 and ans2 == correct_ans2:\n",
    "                                correct += 1\n",
    "                                result_parts.append(f\"‚úÖ **Q2:** Correct! ({ans2})\")\n",
    "                            else:\n",
    "                                result_parts.append(f\"‚ùå **Q2:** Wrong. You answered: {ans2 or 'None'}\")\n",
    "                                result_parts.append(f\"    üí° **Correct answer:** {correct_ans2}\")\n",
    "                            \n",
    "                            user_state.points += correct * 5\n",
    "                            \n",
    "                            # Overall result\n",
    "                            score_emoji = \"üéâ\" if correct == 2 else \"üëç\" if correct == 1 else \"üìö\"\n",
    "                            result_parts.insert(0, f\"{score_emoji} **Final Score: {correct}/2 correct! +{correct*5} XP**\")\n",
    "                            result_parts.append(f\"\\nüèÜ **Total XP:** {user_state.points}\")\n",
    "                            \n",
    "                            return \"\\n\".join(result_parts), update_progress()\n",
    "                        \n",
    "                        gen_quiz_btn.click(generate_quiz_questions, inputs=quiz_topic,\n",
    "                                         outputs=[quiz_q1, quiz_r1, quiz_q2, quiz_r2, submit_btn, quiz_state])\n",
    "                        submit_btn.click(evaluate_quiz, inputs=[quiz_r1, quiz_r2, quiz_state], outputs=[quiz_result, progress])\n",
    "                    \n",
    "                    # === CODING PUZZLES ===\n",
    "                    with gr.Tab(\"üíª Coding\"):\n",
    "                        gr.Markdown(\"\"\"\n",
    "### üöÄ **GPU Speedup Challenge**\n",
    "Convert pandas code to cuDF and witness the power of GPU acceleration!\n",
    "\n",
    "**üéØ Your Mission:**\n",
    "1. Choose difficulty level\n",
    "2. Convert CPU (pandas) code to GPU (cuDF)\n",
    "3. **See real-time performance benchmarking** with profiling analysis!\n",
    "\n",
    "**‚ö° Expected Performance Gains:**\n",
    "- **Beginner:** 6x speedup\n",
    "- **Intermediate:** 6.4x speedup  \n",
    "- **Advanced:** 7x speedup\n",
    "\n",
    "**üî¨ Enhanced Benchmarking Features:**\n",
    "- **Real-time execution** with `perf_counter` precision timing\n",
    "- **Code profiling** with `cProfile` for detailed function analysis\n",
    "- **Live performance comparison** between your code and reference\n",
    "- **Educational insights** about when GPU acceleration helps vs. hurts\n",
    "\"\"\")\n",
    "                        difficulty = gr.Radio(choices=[\"Beginner\", \"Intermediate\", \"Advanced\"], \n",
    "                                            value=\"Beginner\", label=\"Difficulty\")\n",
    "                        gen_puzzle_btn = gr.Button(\"üéØ Generate Puzzle\")\n",
    "                        \n",
    "                        puzzle_desc = gr.Markdown()\n",
    "                        cpu_code = gr.Code(label=\"CPU Code (pandas)\", interactive=False)\n",
    "                        user_code = gr.Code(label=\"Your GPU Code (cuDF)\", language=\"python\")\n",
    "                        \n",
    "                        with gr.Row():\n",
    "                            check_btn = gr.Button(\"‚úÖ Check Solution\", variant=\"secondary\")\n",
    "                            benchmark_btn = gr.Button(\"‚ö° LIVE BENCHMARK üî¨\", variant=\"primary\", size=\"lg\", elem_classes=\"benchmark-btn\")\n",
    "                        \n",
    "                        gr.Markdown(\"### üìä **Performance Results:**\")\n",
    "                        puzzle_feedback = gr.Markdown()\n",
    "                        puzzle_state = gr.State({})\n",
    "                        \n",
    "                        def generate_puzzle(diff):\n",
    "                            puzzle = generate_coding_puzzle(diff)\n",
    "                            expected_speedup = round(puzzle['cpu_time'] / puzzle['gpu_time'], 1)\n",
    "                            \n",
    "                            desc = f\"\"\"**üéØ {diff} Challenge:** {puzzle['description']}\n",
    "\n",
    "üöÄ **Expected Speedup:** {expected_speedup}x faster with GPU acceleration!\n",
    "‚è±Ô∏è **Performance Target:** {puzzle['cpu_time']}s ‚Üí {puzzle['gpu_time']}s\n",
    "\n",
    "üí° **Goal:** Convert the pandas code below to cuDF and unlock GPU performance!\"\"\"\n",
    "                            return desc, puzzle['cpu_code'], \"\", puzzle\n",
    "                        \n",
    "                        def check_solution(user_code_input, puzzle):\n",
    "                            if not puzzle or not user_code_input:\n",
    "                                return \"Generate a puzzle and enter your solution!\", update_progress()\n",
    "                            \n",
    "                            # Comprehensive code validation\n",
    "                            def validate_cudf_solution(user_code, expected_code):\n",
    "                                user_code = user_code.strip()\n",
    "                                expected_code = expected_code.strip()\n",
    "                                \n",
    "                                # Check 1: Basic syntax validation\n",
    "                                try:\n",
    "                                    compile(user_code, '<string>', 'exec')\n",
    "                                except SyntaxError as e:\n",
    "                                    return False, f\"‚ùå **Syntax Error:** {str(e)}\\n\\nüí° Check your Python syntax!\"\n",
    "                                \n",
    "                                # Check 2: Required imports\n",
    "                                if 'import cudf' not in user_code and 'from cudf' not in user_code:\n",
    "                                    return False, \"‚ùå **Missing cuDF import!**\\n\\nüí° You must import cudf to use GPU acceleration.\"\n",
    "                                \n",
    "                                # Check 3: No pandas usage (common mistake)\n",
    "                                if 'import pandas' in user_code or 'pd.' in user_code:\n",
    "                                    return False, \"‚ùå **Still using pandas!**\\n\\nüí° Replace all pandas/pd references with cudf.\"\n",
    "                                \n",
    "                                # Check 4: Required cuDF operations\n",
    "                                expected_ops = []\n",
    "                                if 'cudf.DataFrame' in expected_code:\n",
    "                                    expected_ops.append('cudf.DataFrame')\n",
    "                                if '.groupby(' in expected_code:\n",
    "                                    expected_ops.append('.groupby(')\n",
    "                                if '.sum()' in expected_code:\n",
    "                                    expected_ops.append('.sum()')\n",
    "                                if '.agg(' in expected_code:\n",
    "                                    expected_ops.append('.agg(')\n",
    "                                if '.mean()' in expected_code:\n",
    "                                    expected_ops.append('.mean()')\n",
    "                                \n",
    "                                missing_ops = [op for op in expected_ops if op not in user_code]\n",
    "                                if missing_ops:\n",
    "                                    return False, f\"‚ùå **Missing operations:** {', '.join(missing_ops)}\\n\\nüí° Your solution should include all required operations.\"\n",
    "                                \n",
    "                                # Check 5: Similarity check as final validation\n",
    "                                similarity = difflib.SequenceMatcher(None, user_code, expected_code).ratio()\n",
    "                                if similarity < 0.8:  # Stricter threshold\n",
    "                                    return False, f\"‚ùå **Solution not close enough to expected pattern.**\\n\\nüí° **Your similarity:** {similarity:.2f} (need ‚â•0.80)\\n\\n**Expected pattern:**\\n```python\\n{expected_code}\\n```\"\n",
    "                                \n",
    "                                return True, f\"‚úÖ **Valid cuDF solution!** (similarity: {similarity:.2f})\"\n",
    "                            \n",
    "                            # Validate the user's solution\n",
    "                            is_valid, feedback = validate_cudf_solution(user_code_input, puzzle['gpu_code'])\n",
    "                            \n",
    "                            if is_valid:\n",
    "                                # Only award points for truly valid solutions\n",
    "                                similarity = difflib.SequenceMatcher(None, \n",
    "                                                                   user_code_input.strip(), \n",
    "                                                                   puzzle['gpu_code'].strip()).ratio()\n",
    "                                \n",
    "                                if similarity > 0.95:\n",
    "                                    user_state.points += 10\n",
    "                                    return f\"üéâ **Perfect solution!** +10 XP | Total: {user_state.points} XP\\n\\n{feedback}\", update_progress()\n",
    "                                elif similarity > 0.85:\n",
    "                                    user_state.points += 8\n",
    "                                    return f\"üåü **Excellent solution!** +8 XP | Total: {user_state.points} XP\\n\\n{feedback}\", update_progress()\n",
    "                                elif similarity > 0.80:\n",
    "                                    user_state.points += 5\n",
    "                                    return f\"‚úÖ **Good solution!** +5 XP | Total: {user_state.points} XP\\n\\n{feedback}\", update_progress()\n",
    "                                else:\n",
    "                                    # This should rarely happen due to validation above\n",
    "                                    user_state.points += 3\n",
    "                                    return f\"üëç **Acceptable solution!** +3 XP | Total: {user_state.points} XP\\n\\n{feedback}\", update_progress()\n",
    "                            else:\n",
    "                                # No points for invalid solutions\n",
    "                                return f\"{feedback}\\n\\n**üîç Debug your code step by step:**\\n1. Check imports (use `import cudf`)\\n2. Replace `pd.` with `cudf.`\\n3. Ensure all operations match the expected pattern\", update_progress()\n",
    "                        \n",
    "                        def run_benchmark(user_code_input, puzzle):\n",
    "                            if not puzzle:\n",
    "                                return \"Generate a puzzle first!\"\n",
    "                            \n",
    "                            # Import required timing and profiling modules\n",
    "                            import time\n",
    "                            import io\n",
    "                            import sys\n",
    "                            import cProfile\n",
    "                            import pstats\n",
    "                            from contextlib import redirect_stdout, redirect_stderr\n",
    "                            \n",
    "                            # Use the same validation logic as check_solution\n",
    "                            def validate_cudf_solution(user_code, expected_code):\n",
    "                                user_code = user_code.strip()\n",
    "                                expected_code = expected_code.strip()\n",
    "                                \n",
    "                                # Check 1: Basic syntax validation\n",
    "                                try:\n",
    "                                    compile(user_code, '<string>', 'exec')\n",
    "                                except SyntaxError:\n",
    "                                    return False\n",
    "                                \n",
    "                                # Check 2: Required imports\n",
    "                                if 'import cudf' not in user_code and 'from cudf' not in user_code:\n",
    "                                    return False\n",
    "                                \n",
    "                                # Check 3: No pandas usage (common mistake)\n",
    "                                if 'import pandas' in user_code or 'pd.' in user_code:\n",
    "                                    return False\n",
    "                                \n",
    "                                # Check 4: Required cuDF operations\n",
    "                                expected_ops = []\n",
    "                                if 'cudf.DataFrame' in expected_code:\n",
    "                                    expected_ops.append('cudf.DataFrame')\n",
    "                                if '.groupby(' in expected_code:\n",
    "                                    expected_ops.append('.groupby(')\n",
    "                                if '.sum()' in expected_code:\n",
    "                                    expected_ops.append('.sum()')\n",
    "                                if '.agg(' in expected_code:\n",
    "                                    expected_ops.append('.agg(')\n",
    "                                if '.mean()' in expected_code:\n",
    "                                    expected_ops.append('.mean()')\n",
    "                                \n",
    "                                missing_ops = [op for op in expected_ops if op not in user_code]\n",
    "                                if missing_ops:\n",
    "                                    return False\n",
    "                                \n",
    "                                # Check 5: Similarity check as final validation\n",
    "                                similarity = difflib.SequenceMatcher(None, user_code, expected_code).ratio()\n",
    "                                return similarity >= 0.8\n",
    "                            \n",
    "                            # Validate before showing benchmark\n",
    "                            is_valid = validate_cudf_solution(user_code_input, puzzle['gpu_code'])\n",
    "                            \n",
    "                            if not is_valid:\n",
    "                                return \"‚ùå **Fix your solution first before benchmarking!**\\n\\nüí° Your code must:\\n‚Ä¢ Have valid Python syntax\\n‚Ä¢ Import cudf (not pandas)\\n‚Ä¢ Use cuDF operations correctly\\n‚Ä¢ Match the expected pattern closely\\n\\n**Click 'Check Solution' first to get detailed feedback.**\"\n",
    "                            \n",
    "                            def execute_and_time(code_str, description, is_gpu_code=False):\n",
    "                                \"\"\"Execute code and return timing results with profiling and device handling\"\"\"\n",
    "                                try:\n",
    "                                    # Create a clean namespace for execution\n",
    "                                    namespace = {}\n",
    "                                    \n",
    "                                    # Handle device-specific code execution\n",
    "                                    if is_gpu_code and not device_info['cudf_available']:\n",
    "                                        # Convert cuDF code to pandas for fallback\n",
    "                                        fallback_code = code_str.replace('import cudf', 'import pandas as pd')\n",
    "                                        fallback_code = fallback_code.replace('cudf.', 'pd.')\n",
    "                                        print(f\"‚ö†Ô∏è cuDF not available - using pandas fallback for {description}\")\n",
    "                                        code_to_execute = fallback_code\n",
    "                                        actual_description = f\"{description} (pandas fallback)\"\n",
    "                                    else:\n",
    "                                        code_to_execute = code_str\n",
    "                                        actual_description = description\n",
    "                                    \n",
    "                                    # Add device information to namespace for debugging\n",
    "                                    namespace['__device_info__'] = device_info\n",
    "                                    \n",
    "                                    # Redirect stdout to capture print statements\n",
    "                                    captured_output = io.StringIO()\n",
    "                                    \n",
    "                                    # Timing with perf_counter for high precision\n",
    "                                    start_time = time.perf_counter()\n",
    "                                    \n",
    "                                    # Profile the execution\n",
    "                                    profiler = cProfile.Profile()\n",
    "                                    profiler.enable()\n",
    "                                    \n",
    "                                    # Execute the code with output capture\n",
    "                                    with redirect_stdout(captured_output), redirect_stderr(captured_output):\n",
    "                                        exec(code_to_execute, namespace)\n",
    "                                    \n",
    "                                    profiler.disable()\n",
    "                                    end_time = time.perf_counter()\n",
    "                                    \n",
    "                                    # Calculate execution time\n",
    "                                    execution_time = end_time - start_time\n",
    "                                    \n",
    "                                    # Get profiling stats\n",
    "                                    stats_stream = io.StringIO()\n",
    "                                    stats = pstats.Stats(profiler, stream=stats_stream)\n",
    "                                    stats.sort_stats('cumulative').print_stats(5)  # Top 5 functions\n",
    "                                    \n",
    "                                    # Add device information to the result\n",
    "                                    return {\n",
    "                                        'time': execution_time,\n",
    "                                        'output': captured_output.getvalue(),\n",
    "                                        'profiling': stats_stream.getvalue(),\n",
    "                                        'success': True,\n",
    "                                        'description': actual_description,\n",
    "                                        'using_fallback': is_gpu_code and not device_info['cudf_available'],\n",
    "                                        'device': device_info['cudf_device'] if is_gpu_code else 'CPU'\n",
    "                                    }\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    error_msg = str(e)\n",
    "                                    # Provide specific help for cuDF-related errors\n",
    "                                    if 'cudf' in error_msg.lower() and not device_info['cudf_available']:\n",
    "                                        error_msg += \"\\nüí° Hint: cuDF requires GPU and proper installation. Using pandas fallback.\"\n",
    "                                    \n",
    "                                    return {\n",
    "                                        'time': float('inf'),\n",
    "                                        'output': f\"Error: {error_msg}\",\n",
    "                                        'profiling': \"\",\n",
    "                                        'success': False,\n",
    "                                        'description': description,\n",
    "                                        'using_fallback': is_gpu_code and not device_info['cudf_available'],\n",
    "                                        'device': 'Error'\n",
    "                                    }\n",
    "                            \n",
    "                            # Run benchmarks for both CPU and GPU code\n",
    "                            print(\"üî• Running real-time benchmarks...\")\n",
    "                            \n",
    "                            # Benchmark CPU code (pandas)\n",
    "                            cpu_result = execute_and_time(puzzle['cpu_code'], \"CPU (pandas)\", is_gpu_code=False)\n",
    "                            \n",
    "                            # Benchmark GPU code (user's cuDF solution)\n",
    "                            gpu_result = execute_and_time(user_code_input.strip(), \"GPU (cuDF)\", is_gpu_code=True)\n",
    "                            \n",
    "                            # Check if both executions were successful\n",
    "                            if not cpu_result['success']:\n",
    "                                return f\"‚ùå **CPU code execution failed:** {cpu_result['output']}\"\n",
    "                            \n",
    "                            if not gpu_result['success']:\n",
    "                                return f\"‚ùå **Your GPU code execution failed:** {gpu_result['output']}\\n\\nüí° Debug your code and try again.\"\n",
    "                            \n",
    "                            # Calculate actual speedup\n",
    "                            if gpu_result['time'] > 0:\n",
    "                                actual_speedup = cpu_result['time'] / gpu_result['time']\n",
    "                            else:\n",
    "                                actual_speedup = float('inf')\n",
    "                            \n",
    "                            # Get similarity score\n",
    "                            similarity = difflib.SequenceMatcher(None, \n",
    "                                                               user_code_input.strip(), \n",
    "                                                               puzzle['gpu_code'].strip()).ratio()\n",
    "                            \n",
    "                            # Use actual timing data\n",
    "                            cpu_time = cpu_result['time']\n",
    "                            gpu_time = gpu_result['time']\n",
    "                            speedup = round(actual_speedup, 2)\n",
    "                            \n",
    "                            # Create visual speedup comparison\n",
    "                            cpu_bar_width = 100\n",
    "                            gpu_bar_width = max(10, int(100 / max(speedup, 0.1)))\n",
    "                            \n",
    "                            # Generate ASCII chart with real timings\n",
    "                            ascii_chart = create_speedup_visualization(cpu_time, gpu_time, speedup)\n",
    "                            \n",
    "                            # Determine speedup performance level\n",
    "                            if speedup >= 5:\n",
    "                                performance_emoji = \"üî•\"\n",
    "                                performance_text = \"INCREDIBLE\"\n",
    "                                color = \"#ff6b35\"\n",
    "                            elif speedup >= 3:\n",
    "                                performance_emoji = \"‚ö°\"\n",
    "                                performance_text = \"EXCELLENT\"\n",
    "                                color = \"#f7931e\"\n",
    "                            elif speedup >= 2:\n",
    "                                performance_emoji = \"üöÄ\"\n",
    "                                performance_text = \"GREAT\"\n",
    "                                color = \"#ffd23f\"\n",
    "                            elif speedup >= 1:\n",
    "                                performance_emoji = \"üìà\"\n",
    "                                performance_text = \"GOOD\"\n",
    "                                color = \"#06d6a0\"\n",
    "                            else:\n",
    "                                performance_emoji = \"‚ö†Ô∏è\"\n",
    "                                performance_text = \"SLOWER\"\n",
    "                                color = \"#e74c3c\"\n",
    "                            \n",
    "                            # Format timing data for display\n",
    "                            def format_time(t):\n",
    "                                if t < 1e-6:\n",
    "                                    return f\"{t*1e9:.2f}ns\"\n",
    "                                elif t < 1e-3:\n",
    "                                    return f\"{t*1e6:.2f}Œºs\"\n",
    "                                elif t < 1:\n",
    "                                    return f\"{t*1e3:.2f}ms\"\n",
    "                                else:\n",
    "                                    return f\"{t:.4f}s\"\n",
    "                            \n",
    "                            # Extract key profiling information\n",
    "                            cpu_profile_lines = cpu_result['profiling'].split('\\n')[:10]  # First 10 lines\n",
    "                            gpu_profile_lines = gpu_result['profiling'].split('\\n')[:10]  # First 10 lines\n",
    "                            \n",
    "                            # Determine device status and create appropriate messaging\n",
    "                            gpu_device_status = \"üöÄ GPU (cuDF)\" if not gpu_result.get('using_fallback', False) else \"‚ö†Ô∏è GPU (pandas fallback)\"\n",
    "                            device_warning = \"\"\n",
    "                            \n",
    "                            if gpu_result.get('using_fallback', False):\n",
    "                                device_warning = f\"\"\"\n",
    "### ‚ö†Ô∏è **DEVICE STATUS NOTICE:**\n",
    "**cuDF Availability:** {device_info['cudf_available']} ({'Available' if device_info['cudf_available'] else 'Not Available'})\n",
    "**GPU Status:** {device_info['has_gpu']} ({'Available' if device_info['has_gpu'] else 'Not Available'})\n",
    "**Fallback Mode:** Using pandas simulation for GPU code\n",
    "**Impact:** Results may not reflect true GPU acceleration performance\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "                            \n",
    "                            benchmark_html = f\"\"\"\n",
    "# üèÅ **REAL-TIME BENCHMARK RESULTS**\n",
    "\n",
    "## üîß **SYSTEM CONFIGURATION:**\n",
    "- **Main Device:** {device_info['device_name']}\n",
    "- **GPU Available:** {'‚úÖ Yes' if device_info['has_gpu'] else '‚ùå No'}\n",
    "- **cuDF Status:** {'‚úÖ Available' if device_info['cudf_available'] else '‚ùå Not Available'}\n",
    "- **Execution Mode:** {'Real GPU acceleration' if device_info['cudf_available'] else 'Pandas simulation'}\n",
    "\n",
    "{device_warning}\n",
    "\n",
    "## ‚ö° **LIVE PERFORMANCE ANALYSIS**\n",
    "### {performance_emoji} **{speedup}x {'FASTER' if speedup >= 1 else 'SLOWER'}** - {performance_text} PERFORMANCE!\n",
    "\n",
    "{ascii_chart}\n",
    "\n",
    "---\n",
    "\n",
    "### ‚è±Ô∏è **Precise Timing Results (perf_counter):**\n",
    "\n",
    "**üñ•Ô∏è CPU (pandas) Execution:**\n",
    "- **Device:** {cpu_result.get('device', 'CPU')}\n",
    "- **Time:** `{format_time(cpu_time)}`\n",
    "- **Output:** `{cpu_result['output'][:100]}{'...' if len(cpu_result['output']) > 100 else ''}`\n",
    "\n",
    "**{gpu_device_status} Execution:**\n",
    "- **Device:** {gpu_result.get('device', 'Unknown')}\n",
    "- **Time:** `{format_time(gpu_time)}`\n",
    "- **Output:** `{gpu_result['output'][:100]}{'...' if len(gpu_result['output']) > 100 else ''}`\n",
    "- **Using Fallback:** {'Yes (pandas simulation)' if gpu_result.get('using_fallback', False) else 'No (real cuDF)'}\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Visual Performance Comparison:**\n",
    "\n",
    "**üñ•Ô∏è CPU (pandas):**\n",
    "<div style='background: linear-gradient(90deg, #e74c3c, #c0392b); height: 25px; width: {cpu_bar_width}%; border-radius: 12px; color: white; text-align: center; line-height: 25px; font-weight: bold; margin: 5px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.2);'>\n",
    "{format_time(cpu_time)}\n",
    "</div>\n",
    "\n",
    "**{gpu_device_status}:**\n",
    "<div style='background: linear-gradient(90deg, {\"#f39c12, #e67e22\" if gpu_result.get(\"using_fallback\", False) else \"#27ae60, #2ecc71\"}); height: 25px; width: {gpu_bar_width}%; border-radius: 12px; color: white; text-align: center; line-height: 25px; font-weight: bold; margin: 5px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.2);'>\n",
    "{format_time(gpu_time)}\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ **Profiling Analysis (Top Functions):**\n",
    "\n",
    "**CPU Profiling:**\n",
    "```\n",
    "{''.join(cpu_profile_lines)}\n",
    "```\n",
    "\n",
    "**GPU Profiling:**\n",
    "```\n",
    "{''.join(gpu_profile_lines)}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Detailed Metrics:**\n",
    "- **‚è±Ô∏è Absolute Time Difference:** {format_time(abs(cpu_time - gpu_time))}\n",
    "- **üìä Percentage Improvement:** {((speedup - 1) * 100):.1f}% {'faster' if speedup >= 1 else 'slower'}\n",
    "- **üéØ Code Similarity:** {similarity:.1%} match with expected solution\n",
    "- **üí∞ Performance Category:** {performance_text}\n",
    "- **üîã Execution Efficiency:** {'GPU parallel processing' if speedup >= 1 else 'CPU sequential processing'} performed better\n",
    "\n",
    "### üìà **Real-World Implications:**\n",
    "- **Current Dataset:** {'GPU' if not gpu_result.get('using_fallback', False) else 'Simulated GPU'} solution is **{speedup:.2f}x {'faster' if speedup >= 1 else 'slower'}**\n",
    "- **Large Dataset (1M+ rows):** {'Could be 10-100x faster with real GPU!' if gpu_result.get('using_fallback', False) else 'Could be 10-100x faster!' if speedup >= 1 else 'Consider pandas for this workload size'}\n",
    "- **Production Scale:** {'Install cuDF and use GPU for massive savings üí∞' if gpu_result.get('using_fallback', False) else 'Massive time and cost savings üí∞' if speedup >= 1 else 'Pandas might be more suitable ü§î'}\n",
    "\n",
    "### üöÄ **Technical Insights:**\n",
    "- **Parallel Processing:** {'GPU acceleration simulated - real GPU would utilize thousands of cores' if gpu_result.get('using_fallback', False) else 'GPU utilizes thousands of cores effectively' if speedup >= 1 else 'CPU single-threaded execution was sufficient'}\n",
    "- **Memory Bandwidth:** {'Real GPU would provide massive memory throughput advantage' if gpu_result.get('using_fallback', False) else 'GPU memory throughput advantage' if speedup >= 1 else 'CPU memory access was adequate'}\n",
    "- **Overhead Analysis:** {'cuDF setup overhead simulated - real GPU overhead often justified for larger datasets' if gpu_result.get('using_fallback', False) else 'cuDF initialization overhead was justified' if speedup >= 1 else 'cuDF overhead exceeded benefits for this dataset size'}\n",
    "\n",
    "{performance_emoji} **{'Set up cuDF and GPU to see real acceleration!' if gpu_result.get('using_fallback', False) else 'Congratulations on mastering GPU acceleration!' if speedup >= 1 else 'Keep practicing - pandas was better for this case!'}** {performance_emoji}\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Learning Notes:**\n",
    "- **Small datasets** (< 10K rows): Pandas often faster due to lower overhead\n",
    "- **Medium datasets** (10K - 1M rows): cuDF starts showing benefits{'üìù Note: Results simulated - real GPU would show even better performance' if gpu_result.get('using_fallback', False) else ''}\n",
    "- **Large datasets** (> 1M rows): cuDF typically dominates{'üöÄ Real GPU acceleration would be dramatic here!' if gpu_result.get('using_fallback', False) else ''}\n",
    "- **Your result:** {performance_text.lower()} performance indicates {'simulation mode - real GPU would likely perform much better' if gpu_result.get('using_fallback', False) else 'optimal GPU utilization' if speedup >= 2 else 'room for optimization' if speedup >= 1 else 'pandas was the right choice here'}\n",
    "\n",
    "### üîß **Setup Recommendations:**\n",
    "{\n",
    "f'''\n",
    "- **Install cuDF:** `conda install -c rapidsai -c conda-forge -c nvidia cudf python=3.10 cudatoolkit=11.8`\n",
    "- **GPU Requirements:** NVIDIA GPU with CUDA support\n",
    "- **Memory:** At least 8GB GPU memory recommended\n",
    "- **Alternative:** Use Google Colab with GPU runtime for testing\n",
    "''' if gpu_result.get('using_fallback', False) else '''\n",
    "- **Optimization:** Your GPU setup is working perfectly!\n",
    "- **Next Steps:** Try larger datasets to see dramatic speedups\n",
    "- **Advanced:** Explore cuML for machine learning acceleration\n",
    "'''\n",
    "}\n",
    "\"\"\"\n",
    "                            return benchmark_html\n",
    "                        \n",
    "                        gen_puzzle_btn.click(generate_puzzle, inputs=difficulty, \n",
    "                                           outputs=[puzzle_desc, cpu_code, user_code, puzzle_state])\n",
    "                        check_btn.click(check_solution, inputs=[user_code, puzzle_state], outputs=[puzzle_feedback, progress])\n",
    "                        benchmark_btn.click(run_benchmark, inputs=[user_code, puzzle_state], outputs=puzzle_feedback)\n",
    "                \n",
    "                # Update progress on point changes\n",
    "                def refresh_progress():\n",
    "                    return update_progress()\n",
    "        \n",
    "        # Footer\n",
    "        gr.Markdown(\"\"\"\n",
    "        ---\n",
    "        <div style='text-align:center; color:#666; padding:15px;'>\n",
    "        üåü **AI Tutor Complete Learning System** ‚Ä¢ Structured Learning Paths ‚Ä¢ Socratic Guidance ‚Ä¢ Gamified Practice<br>\n",
    "        üìö <i>From beginner tutorials to advanced GPU computing - your complete data science learning companion!</i><br>\n",
    "        üí° **Learning Mode:** Topic-based structured courses | **Tutor Mode:** Socratic Q&A | **Game Mode:** XP-based practice\n",
    "        </div>\n",
    "        \"\"\")\n",
    "    \n",
    "    return app\n",
    "\n",
    "# === LAUNCH ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ Creating AI Tutor interface...\")\n",
    "    app = create_ai_tutor()\n",
    "    \n",
    "    print(\"üéâ AI Tutor ready!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ Complete Learning System Available:\")\n",
    "    print(\"  üìö Learning Mode - Structured topic-based learning paths\")\n",
    "    print(\"  üéì Tutor Mode - Socratic Q&A with guided discovery\")\n",
    "    print(\"  üéÆ Game Mode - XP-based practice with flashcards, quizzes & coding\")\n",
    "    print(\"  üåü Dynamic content generation with contextual quotes\")\n",
    "    print(\"  üìñ Enhanced documentation (12+ sources)\")\n",
    "    print(\"  üéØ From beginner Python to advanced GPU computing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Launch the app\n",
    "    app.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
